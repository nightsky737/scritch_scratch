{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24a9cc12-2578-4aef-8ee6-884ffab4f3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from grad import *\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc4ff80a-6930-4829-aa43-4236da9d571d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_loss(truth, model):\n",
    "    ''' Simple L2 loss for a single sample.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        truth : Sequence[float]\n",
    "            A single data point.\n",
    "              \n",
    "        model : Sequence[float]\n",
    "            The parameters of a model.\n",
    "              \n",
    "        Returns\n",
    "        -------\n",
    "        Number : the L2 loss (squared error) of `model_params` evaluated on `truth`\n",
    "    '''\n",
    "    l = truth[0] - model[0] - sum(truth[i]*model[i] for i in range(1, len(model)))\n",
    "    return l**2 if l.data > 0 else (-1*l)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d30f6ee-fae9-4977-9c0d-a794d25066a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topo_sort(N):\n",
    "    '''Returns the correct order to backprop N in. Yes. This is such that if you call backprop(N), the gradient on N is final.\n",
    "    rets a list of Numbers.\n",
    "    \n",
    "    So ideally we'd just backprop one layer (to N's two creators (ie a and b when u backprop N in a * b = N) when we get to the N \n",
    "\n",
    "    *sigh* back to the usaco grind\n",
    "    '''\n",
    "    ret = []\n",
    "    visited = {}\n",
    "    def dfs(number):\n",
    "        '''\n",
    "        dfs's a Number, following it down the path by df'sing its two creators.\n",
    "        '''\n",
    "        if visited.get(number)== \"visited\":\n",
    "            return True\n",
    "        if visited.get(number) == \"visiting\":\n",
    "            return False\n",
    "        visited[number] = \"visiting\"\n",
    "        \n",
    "        if number.creator != None:\n",
    "            if not dfs(number.creator.a):\n",
    "                print(number, \" to \" , number.creator.a)\n",
    "            if not dfs(number.creator.b):\n",
    "                print(number, \" to \" , number.creator.b)\n",
    "        visited[number] = \"visited\"\n",
    "        ret.append(number)\n",
    "        return True\n",
    "        \n",
    "    dfs(N)\n",
    "    ret.reverse()\n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f21584e9-684c-41a7-b705-dd07612aae02",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "def weight_matrix(shape, naive=False):\n",
    "    \"\"\"weight matrix thingy.give dims. Not 0.\"\"\"\n",
    "    # np.as_array(Number(np.random.rand()) for i in range(M*N))\n",
    "    number = 1\n",
    "    if(type(shape) == int):\n",
    "        shape = [shape]\n",
    "    for i in shape:\n",
    "        number*= i\n",
    "    if naive:\n",
    "        return np.array([Number(i / 10) for i in range(number)]).reshape(*shape)\n",
    "    return np.array([Number(np.random.uniform(low=-.2, high=.2, size=None)) for i in range(number)]).reshape(*shape)\n",
    " \n",
    "def get_grads(x):\n",
    "    vectorized_grad = np.vectorize(lambda x : x.grad) #not true vecotrization just for readibility\n",
    "    return vectorized_grad(x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return np.vectorize(lambda x: 1/(1+math.e**-x))(x)\n",
    "\n",
    "def relu(x):\n",
    "    # print(x)\n",
    "    def relu_one(k):\n",
    "        if k <= 0:\n",
    "            return 10e-2 * k\n",
    "        else:\n",
    "            return k\n",
    "    return np.vectorize(lambda x: relu_one(x))(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "c1818126-0bf2-4043-afd4-49f3025297f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    '''this only works w/ MSE and sigmoid because dik how to topological sort'''\n",
    "    \n",
    "    def __init__(self, input_size, output_size, hidden_layers, naive=False):\n",
    "        '''\n",
    "        Takes list of # of things in their layers.\n",
    "        Layers are outputs?\n",
    "        '''\n",
    "        self.layer_sizes = hidden_layers\n",
    "        self.layers = []\n",
    "        self.biases = []\n",
    "\n",
    "        #Hidden states is after the *weight but before sigmoid\n",
    "        self.hidden_states = []\n",
    "\n",
    "        self.hidden_states_sigmoid = []\n",
    "        \n",
    "        prev = input_size\n",
    "        for hidden_layer in hidden_layers:\n",
    "            # self.layers, weight_matrix([prev, hidden_layer])\n",
    "            self.layers.append(weight_matrix([prev, hidden_layer], naive))\n",
    "\n",
    "            self.biases.append(weight_matrix(hidden_layer, naive))\n",
    "            prev  = hidden_layer\n",
    "        self.biases.append(weight_matrix([output_size]))\n",
    "        self.layers.append(weight_matrix([prev, output_size]))\n",
    "  \n",
    "    def fd(self, x):\n",
    "        '''f pass with input. Input has to be flat like a pancake'''\n",
    "        #sigma sigma boy.\n",
    "        self.hidden_states_sigmoid = []\n",
    "        self.hidden_states = []\n",
    "        self.input = x\n",
    "        for i in range(len(self.layers)):\n",
    "            # print(np.max(x))\n",
    "            x = x @ self.layers[i]\n",
    "            x += self.biases[i]\n",
    "            self.hidden_states.append(x)\n",
    "            if i != len(self.layers) - 1:\n",
    "                x = relu(x)\n",
    "                # print(\"Non-zero ReLUs:\", np.count_nonzero(x))\n",
    "            else:\n",
    "                x = sigmoid(x)\n",
    "            self.hidden_states_sigmoid.append(x)\n",
    "\n",
    "        return x\n",
    "        \n",
    "    def train_epoch(self, x, y, lr=10**-2):\n",
    "        '''\n",
    "        f pass and then uh gradient descent?\n",
    "\n",
    "        x: Input. Again, flat as a pancake.\n",
    "        y: the goal. In sparese tensor. \n",
    "        lr: how quick it learns\n",
    "        '''\n",
    "        num_correct = 0\n",
    "        losses = []\n",
    "        for i in range(len(y)):\n",
    "        #     for layer in self.layers:\n",
    "        #         for w in layer.flat:\n",
    "        #             w.null_gradients(recursive=False)\n",
    "\n",
    "        #     for bias in self.biases:\n",
    "        #         for b in bias.flat:\n",
    "        #             b.null_gradients(recursive=False)\n",
    "                    \n",
    "            pred = self.fd(x[i])\n",
    "            # mse = np.sum((pred * pred +  y[i] *  y[i] - 2 * np.dot(pred, y[i]))) / len(y)\n",
    "            mse = sum((a - b)**2 for a, b in zip(pred, y[i]))\n",
    "\n",
    "            num_correct += np.argmax(pred) == np.argmax(y[i])\n",
    "            losses.append(mse)\n",
    "            mse_sorted = topo_sort(mse)\n",
    "            # print(f\"order {mse_sorted}\")\n",
    "            for num in mse_sorted:\n",
    "                num.backprop_single()\n",
    "            # print(\"Final layer pre-sigmoid:\", self.hidden_states[-1])\n",
    "\n",
    "            #These look ok.\n",
    "            # for i, layer in enumerate(self.layers):\n",
    "            #     for j, num in enumerate(layer.flat):\n",
    "            #         print(f\"Layer {i} Flat[{j}] grad: {num.grad}\")\n",
    "            #     for m, r in enumerate(layer):\n",
    "            #         for n, c in enumerate(r):\n",
    "            #             print(f\"Layer {i} Row {m} col {n} grad: {c.grad}\")\n",
    "            # self.print_info()\n",
    "            \n",
    "            #What in the FREAK is nulling the gradients here?????\n",
    "            for i, layer in enumerate(self.layers):\n",
    "                for w in range(len(layer.flat)):\n",
    "                    layer.flat[w] = Number(layer.flat[w] - layer.flat[w].grad * lr)\n",
    "            #OH WAIT IT SHOULD BE NULL :sob: WE MAKING NEW NUMBERS. Cleans the graph too!\n",
    "            # self.print_info()\n",
    "\n",
    "            for i in range(len(self.biases)):\n",
    "                layer = self.biases[i]\n",
    "                for b in range(len(layer.flat)):\n",
    "                    layer.flat[b] = Number( layer.flat[b] - layer.flat[b].grad * lr)\n",
    "        print(f\"Acc: {num_correct/len(y)} Avg loss: {sum(losses)/len(y)}\")\n",
    "            # print(f\"Acc: {num_correct/len(y)} Avg loss: {losses}\")\n",
    "\n",
    "    def print_info(self, verbose=True):\n",
    "        print(\"layers \" )\n",
    "        for i in range(len(self.layers)):\n",
    "            print( f\"weight {i} of shape {self.layers[i].shape}\")\n",
    "            print(self.layers[i])\n",
    "            print(\"grads\")\n",
    "            print(np.vectorize(lambda x : x.grad)(self.layers[i]))\n",
    "            \n",
    "        print(\"biases \")\n",
    "        for i in range(len(self.biases)):\n",
    "            print( f\"bias {i} of shape {self.biases[i].shape}\")\n",
    "            print(self.biases[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "fe3b98f9-02a3-4805-aa29-09b98e9751dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.5 Avg loss: Number(0.5046219601608408)\n"
     ]
    }
   ],
   "source": [
    "tiny_x = [[1, 2], [2, 3]]\n",
    "tiny_test = Model(2, 2, [3, 1], naive=True)\n",
    "tiny_test.train_epoch(tiny_x, np.array( [[0, 1],[1, 0]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "e31d2937-3c69-4fdd-b71a-41185bc395ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.13 Avg loss: Number(1.291816224337269)\n",
      "Acc: 0.09 Avg loss: Number(0.922030117375282)\n",
      "Acc: 0.11 Avg loss: Number(0.9167681954888175)\n",
      "Acc: 0.11 Avg loss: Number(0.9131554217750134)\n",
      "Acc: 0.1 Avg loss: Number(0.9105261004876037)\n",
      "Acc: 0.1 Avg loss: Number(0.9084750528802458)\n",
      "Acc: 0.1 Avg loss: Number(0.9067768847705626)\n",
      "Acc: 0.11 Avg loss: Number(0.905306783956894)\n",
      "Acc: 0.12 Avg loss: Number(0.9039868066098039)\n",
      "Acc: 0.12 Avg loss: Number(0.9027607426123027)\n",
      "Acc: 0.13 Avg loss: Number(0.9015819329495318)\n",
      "Acc: 0.16 Avg loss: Number(0.9004054453628426)\n",
      "Acc: 0.19 Avg loss: Number(0.8991802753332253)\n",
      "Acc: 0.19 Avg loss: Number(0.8978373834696436)\n",
      "Acc: 0.19 Avg loss: Number(0.8962646341611017)\n",
      "Acc: 0.21 Avg loss: Number(0.8942399105721449)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[274], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m plsfuckingwork \u001b[38;5;241m=\u001b[39m Model(\u001b[38;5;241m28\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m10\u001b[39m, [\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m4\u001b[39m])\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m20\u001b[39m):\n\u001b[1;32m----> 4\u001b[0m     plsfuckingwork\u001b[38;5;241m.\u001b[39mtrain_epoch(fixed_x, fixed_y, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10e-2\u001b[39m)\n",
      "Cell \u001b[1;32mIn[270], line 73\u001b[0m, in \u001b[0;36mModel.train_epoch\u001b[1;34m(self, x, y, lr)\u001b[0m\n\u001b[0;32m     71\u001b[0m num_correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(pred) \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(y[i])\n\u001b[0;32m     72\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(mse)\n\u001b[1;32m---> 73\u001b[0m mse_sorted \u001b[38;5;241m=\u001b[39m topo_sort(mse)\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# print(f\"order {mse_sorted}\")\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m num \u001b[38;5;129;01min\u001b[39;00m mse_sorted:\n",
      "Cell \u001b[1;32mIn[5], line 30\u001b[0m, in \u001b[0;36mtopo_sort\u001b[1;34m(N)\u001b[0m\n\u001b[0;32m     27\u001b[0m     ret\u001b[38;5;241m.\u001b[39mappend(number)\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m dfs(N)\n\u001b[0;32m     31\u001b[0m ret\u001b[38;5;241m.\u001b[39mreverse()\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "Cell \u001b[1;32mIn[5], line 22\u001b[0m, in \u001b[0;36mtopo_sort.<locals>.dfs\u001b[1;34m(number)\u001b[0m\n\u001b[0;32m     19\u001b[0m visited[number] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvisiting\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m number\u001b[38;5;241m.\u001b[39mcreator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dfs(number\u001b[38;5;241m.\u001b[39mcreator\u001b[38;5;241m.\u001b[39ma):\n\u001b[0;32m     23\u001b[0m         \u001b[38;5;28mprint\u001b[39m(number, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m to \u001b[39m\u001b[38;5;124m\"\u001b[39m , number\u001b[38;5;241m.\u001b[39mcreator\u001b[38;5;241m.\u001b[39ma)\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dfs(number\u001b[38;5;241m.\u001b[39mcreator\u001b[38;5;241m.\u001b[39mb):\n",
      "Cell \u001b[1;32mIn[5], line 22\u001b[0m, in \u001b[0;36mtopo_sort.<locals>.dfs\u001b[1;34m(number)\u001b[0m\n\u001b[0;32m     19\u001b[0m visited[number] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvisiting\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m number\u001b[38;5;241m.\u001b[39mcreator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dfs(number\u001b[38;5;241m.\u001b[39mcreator\u001b[38;5;241m.\u001b[39ma):\n\u001b[0;32m     23\u001b[0m         \u001b[38;5;28mprint\u001b[39m(number, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m to \u001b[39m\u001b[38;5;124m\"\u001b[39m , number\u001b[38;5;241m.\u001b[39mcreator\u001b[38;5;241m.\u001b[39ma)\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dfs(number\u001b[38;5;241m.\u001b[39mcreator\u001b[38;5;241m.\u001b[39mb):\n",
      "    \u001b[1;31m[... skipping similar frames: topo_sort.<locals>.dfs at line 22 (7 times)]\u001b[0m\n",
      "Cell \u001b[1;32mIn[5], line 24\u001b[0m, in \u001b[0;36mtopo_sort.<locals>.dfs\u001b[1;34m(number)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dfs(number\u001b[38;5;241m.\u001b[39mcreator\u001b[38;5;241m.\u001b[39ma):\n\u001b[0;32m     23\u001b[0m         \u001b[38;5;28mprint\u001b[39m(number, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m to \u001b[39m\u001b[38;5;124m\"\u001b[39m , number\u001b[38;5;241m.\u001b[39mcreator\u001b[38;5;241m.\u001b[39ma)\n\u001b[1;32m---> 24\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dfs(number\u001b[38;5;241m.\u001b[39mcreator\u001b[38;5;241m.\u001b[39mb):\n\u001b[0;32m     25\u001b[0m         \u001b[38;5;28mprint\u001b[39m(number, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m to \u001b[39m\u001b[38;5;124m\"\u001b[39m , number\u001b[38;5;241m.\u001b[39mcreator\u001b[38;5;241m.\u001b[39mb)\n\u001b[0;32m     26\u001b[0m visited[number] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvisited\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "    \u001b[1;31m[... skipping similar frames: topo_sort.<locals>.dfs at line 22 (2 times)]\u001b[0m\n",
      "Cell \u001b[1;32mIn[5], line 24\u001b[0m, in \u001b[0;36mtopo_sort.<locals>.dfs\u001b[1;34m(number)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dfs(number\u001b[38;5;241m.\u001b[39mcreator\u001b[38;5;241m.\u001b[39ma):\n\u001b[0;32m     23\u001b[0m         \u001b[38;5;28mprint\u001b[39m(number, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m to \u001b[39m\u001b[38;5;124m\"\u001b[39m , number\u001b[38;5;241m.\u001b[39mcreator\u001b[38;5;241m.\u001b[39ma)\n\u001b[1;32m---> 24\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dfs(number\u001b[38;5;241m.\u001b[39mcreator\u001b[38;5;241m.\u001b[39mb):\n\u001b[0;32m     25\u001b[0m         \u001b[38;5;28mprint\u001b[39m(number, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m to \u001b[39m\u001b[38;5;124m\"\u001b[39m , number\u001b[38;5;241m.\u001b[39mcreator\u001b[38;5;241m.\u001b[39mb)\n\u001b[0;32m     26\u001b[0m visited[number] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvisited\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "    \u001b[1;31m[... skipping similar frames: topo_sort.<locals>.dfs at line 22 (6 times), topo_sort.<locals>.dfs at line 24 (3 times)]\u001b[0m\n",
      "Cell \u001b[1;32mIn[5], line 24\u001b[0m, in \u001b[0;36mtopo_sort.<locals>.dfs\u001b[1;34m(number)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dfs(number\u001b[38;5;241m.\u001b[39mcreator\u001b[38;5;241m.\u001b[39ma):\n\u001b[0;32m     23\u001b[0m         \u001b[38;5;28mprint\u001b[39m(number, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m to \u001b[39m\u001b[38;5;124m\"\u001b[39m , number\u001b[38;5;241m.\u001b[39mcreator\u001b[38;5;241m.\u001b[39ma)\n\u001b[1;32m---> 24\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dfs(number\u001b[38;5;241m.\u001b[39mcreator\u001b[38;5;241m.\u001b[39mb):\n\u001b[0;32m     25\u001b[0m         \u001b[38;5;28mprint\u001b[39m(number, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m to \u001b[39m\u001b[38;5;124m\"\u001b[39m , number\u001b[38;5;241m.\u001b[39mcreator\u001b[38;5;241m.\u001b[39mb)\n\u001b[0;32m     26\u001b[0m visited[number] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvisited\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "    \u001b[1;31m[... skipping similar frames: topo_sort.<locals>.dfs at line 22 (409 times)]\u001b[0m\n",
      "Cell \u001b[1;32mIn[5], line 22\u001b[0m, in \u001b[0;36mtopo_sort.<locals>.dfs\u001b[1;34m(number)\u001b[0m\n\u001b[0;32m     19\u001b[0m visited[number] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvisiting\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m number\u001b[38;5;241m.\u001b[39mcreator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dfs(number\u001b[38;5;241m.\u001b[39mcreator\u001b[38;5;241m.\u001b[39ma):\n\u001b[0;32m     23\u001b[0m         \u001b[38;5;28mprint\u001b[39m(number, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m to \u001b[39m\u001b[38;5;124m\"\u001b[39m , number\u001b[38;5;241m.\u001b[39mcreator\u001b[38;5;241m.\u001b[39ma)\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dfs(number\u001b[38;5;241m.\u001b[39mcreator\u001b[38;5;241m.\u001b[39mb):\n",
      "Cell \u001b[1;32mIn[5], line 15\u001b[0m, in \u001b[0;36mtopo_sort.<locals>.dfs\u001b[1;34m(number)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdfs\u001b[39m(number):\n\u001b[0;32m     12\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03m    dfs's a Number, following it down the path by df'sing its two creators.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m visited\u001b[38;5;241m.\u001b[39mget(number)\u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvisited\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     16\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m visited\u001b[38;5;241m.\u001b[39mget(number) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvisiting\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\Downloads\\scritch_scratch\\basics\\grad.py:346\u001b[0m, in \u001b[0;36mNumber.__hash__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__eq__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m other\n\u001b[1;32m--> 346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__hash__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    347\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mid\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "5d06d00d-4655-4505-bb07-0f1eeb191763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k1 [Number(1.0) Number(1.1)]\n",
      "k2 [[Number(0.3)]\n",
      " [Number(0.4)]]\n",
      "kb [Number(2.0)]\n",
      "f1 [Number(0.74)]\n",
      "f [Number(2.74)]\n",
      "[Number(2.74), Number(2.0), Number(0.74), Number(0.44000000000000006), Number(0.4), Number(1.1), Number(0.3), Number(0.3), Number(1.0)]\n",
      "a Number(0.74) + Number(0.3) Number(0.44000000000000006)\n"
     ]
    }
   ],
   "source": [
    "fake = np.array([Number(i/10 + 1) for i in range(2)])\n",
    "fake1 = np.array([Number(i/10 + .3) for i in range(2)]).reshape(2,1)\n",
    "fake_bias = np.array([Number(i/10 + 2) for i in range(1)])\n",
    "\n",
    "print(\"k1\" , fake)\n",
    "print(\"k2\" , fake1)\n",
    "print(\"kb\" , fake_bias)\n",
    "\n",
    "final1 = fake @ fake1 \n",
    "final = final1 + fake_bias\n",
    "print(\"f1\" , final1)\n",
    "\n",
    "print(\"f\" , final)\n",
    "\n",
    "idk = topo_sort(final[0])\n",
    "print(idk)\n",
    "for num in final1.flat:\n",
    "    print(\"a\" , num,num.creator, num.creator.a, num.creator.b)\n",
    "\n",
    "for num in final:\n",
    "    \n",
    "    num.backprop_single()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46eb3817-44e3-4ace-977b-f8df662af0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_x = [[1, 2, 3, 4]]\n",
    "tiny_test = Model(4, 1, [3], naive=True)\n",
    "tiny_test.print_info()\n",
    "tiny_test.train_epoch(tiny_x, np.array([2,3]))\n",
    "tiny_test.train_epoch(tiny_x, np.array([2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "734c4959-8b4f-4556-9c4a-26c7ed029f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def null_gradients(layers):\n",
    "    \"\"\"nulls all gradients\"\"\"\n",
    "    for layer in layers:\n",
    "        for weight in layer.flat:\n",
    "            weight.null_gradients()\n",
    "            \n",
    "def check_null(layers):\n",
    "    \"\"\"nulls all gradients\"\"\"\n",
    "    for layer in layers:\n",
    "        for weight in layer.flat:\n",
    "            if weight.grad != None:\n",
    "                print(\"hi welcome to another 5hrs of debuggin\")\n",
    "    print(\"passed null check \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b343a94-8b6b-4bef-b3e6-b86526c5a036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_data(x, y):\n",
    "    x = x.reshape(x.shape[0], 28*28)/255\n",
    "    test = np.zeros((x.shape[0], 10))\n",
    "    test[np.arange(x.shape[0]),y] = 1\n",
    "    return (x, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "e13c3bf4-97dd-4ed4-b50d-dfaaaffa040d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 ... 5 6 8]\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data(path=\"mnist.npz\")\n",
    "testx, testy = fix_data(x_train[0:2], y_train[0:2])\n",
    "rawx = x_train[0:2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "2a38e3b1-1efa-4616-92ad-2887f58d8054",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_x, fixed_y = fix_data(x_train[:1000], y_train[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "58e74a75-3a1e-457d-b458-a4666093bd14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.193 Avg loss: Number(0.9148206999012773)\n",
      "Acc: 0.347 Avg loss: Number(0.7649153700637776)\n",
      "Acc: 0.417 Avg loss: Number(0.7048005599121263)\n",
      "Acc: 0.476 Avg loss: Number(0.659013730140775)\n",
      "Acc: 0.556 Avg loss: Number(0.6126583948084633)\n",
      "Acc: 0.619 Avg loss: Number(0.5233582602451018)\n",
      "Acc: 0.681 Avg loss: Number(0.4554136677357978)\n",
      "Acc: 0.7 Avg loss: Number(0.41836248180804436)\n",
      "Acc: 0.721 Avg loss: Number(0.40502758578499515)\n",
      "Acc: 0.722 Avg loss: Number(0.40834967394121585)\n",
      "Acc: 0.741 Avg loss: Number(0.3860735134187396)\n",
      "Acc: 0.757 Avg loss: Number(0.3879122382040642)\n",
      "Acc: 0.644 Avg loss: Number(0.5298185450036147)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[293], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m plsfuckingwork \u001b[38;5;241m=\u001b[39m Model(\u001b[38;5;241m28\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m10\u001b[39m, [\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m8\u001b[39m])\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n\u001b[1;32m----> 4\u001b[0m     plsfuckingwork\u001b[38;5;241m.\u001b[39mtrain_epoch(fixed_x, fixed_y, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10e-2\u001b[39m)\n",
      "Cell \u001b[1;32mIn[270], line 67\u001b[0m, in \u001b[0;36mModel.train_epoch\u001b[1;34m(self, x, y, lr)\u001b[0m\n\u001b[0;32m     57\u001b[0m losses \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(y)):\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m#     for layer in self.layers:\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m#         for w in layer.flat:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m#         for b in bias.flat:\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m#             b.null_gradients(recursive=False)\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfd(x[i])\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# mse = np.sum((pred * pred +  y[i] *  y[i] - 2 * np.dot(pred, y[i]))) / len(y)\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     mse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m((a \u001b[38;5;241m-\u001b[39m b)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m a, b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(pred, y[i]))\n",
      "Cell \u001b[1;32mIn[270], line 36\u001b[0m, in \u001b[0;36mModel.fd\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers)):\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;66;03m# print(np.max(x))\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[i]\n\u001b[0;32m     37\u001b[0m     x \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbiases[i]\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_states\u001b[38;5;241m.\u001b[39mappend(x)\n",
      "File \u001b[1;32m~\\Downloads\\scritch_scratch\\basics\\grad.py:260\u001b[0m, in \u001b[0;36mNumber.__add__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__add__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[1;32m--> 260\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_op(Add, \u001b[38;5;28mself\u001b[39m, other)\n",
      "File \u001b[1;32m~\\Downloads\\scritch_scratch\\basics\\grad.py:255\u001b[0m, in \u001b[0;36mNumber._op\u001b[1;34m(Op, a, b)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;66;03m#Basically op is a class, and we make an istance of that class of f.\u001b[39;00m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" Get the output of the operation's forward pass, which is an int or float.\u001b[39;00m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;124;03m    Make it ans instance of `Number`, whose creator is f. Return this result.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 255\u001b[0m ans \u001b[38;5;241m=\u001b[39m Number(f(a,b), creator\u001b[38;5;241m=\u001b[39mf)\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ans\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Moment of truth\n",
    "plswork = Model(28*28, 10, [4, 8])\n",
    "for i in range(100):\n",
    "    plswork.train_epoch(fixed_x, fixed_y, lr=10e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915796d0-606b-4d94-bd5a-8d301fb40e51",
   "metadata": {},
   "source": [
    "# still in shock that it actually works "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c865b348-9949-4b77-8749-e95f20828496",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "0fe52250-1221-4ee5-b571-84749f278f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plswork.pred("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e584223e-bcbe-436d-af15-7119a9d07d5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b728fc6c-1552-4dc3-a53e-ca813d84c2dc",
   "metadata": {},
   "source": [
    "# What I learned (future me pls remember this):\n",
    "- Early gradients are kinda random. Don't print out the losses after every single image and then spend get sad when its not decreasing right. Even if it's working, only the after 1 epoch will show decreasing trend.\n",
    "- Number class: w-= w.grad kind of created new Numbers, each of which had gradient = to None. This sent me on a wild goose chase debugging why gradients were fine before w-=w.grad and then None after for a long while.\n",
    "- As another side note, no need to null the gradients if I re-make the graph from scratch (and better not to so that the old weights' states aren't remembered/backproped)\n",
    "- The activation fxn Sigmoid goes at the end (not between layers. Otherwise the gradient gets really small and goes poof)\n",
    "- Weights should also be p small unless I want overflow\n",
    "- Topological sort might have a scary name but it was much easier/cleaner than trying to do it the other way. Moral of the story: scary name is not that bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cad5302-2901-477d-b940-1dd2990543c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
