{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98840bcf-3310-431e-8728-242bb2786b30",
   "metadata": {},
   "source": [
    "# I go through the jax tutorial and attempt to understand 10% of what goes on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c992cbb4-3db8-4812-b1c3-1652b5e88ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import lax\n",
    "from jax import grad\n",
    "import jax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba27783-f449-41a8-b72b-4ae63283c136",
   "metadata": {},
   "source": [
    "- jax.Array Creation: Similar to np.  \n",
    "- There be some complicated stuff about devices that ill get to later\n",
    "- Tracers: Basically u run through fxn with tracer and they help jax \"compile\"/figure out the sequence of operations the fxn carries out (aka the jaxpr)\n",
    "- Pytree: Nested data structures\n",
    "- Jax has random keys that you pass in to fxns instead of np's seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318d1e0a-bdb9-4c80-aab5-f414c6e0e713",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## JIT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "945d50ab-6cc0-40ea-be03-3924fb3fedf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ \u001b[34m\u001b[22m\u001b[1mlambda \u001b[39m\u001b[22m\u001b[22m; a\u001b[35m:f32[]\u001b[39m b\u001b[35m:f32[]\u001b[39m. \u001b[34m\u001b[22m\u001b[1mlet\n",
      "    \u001b[39m\u001b[22m\u001b[22mc\u001b[35m:f32[]\u001b[39m = log a\n",
      "    d\u001b[35m:f32[]\u001b[39m = log b\n",
      "    e\u001b[35m:f32[]\u001b[39m = div c d\n",
      "  \u001b[34m\u001b[22m\u001b[1min \u001b[39m\u001b[22m\u001b[22m(e,) }\n"
     ]
    }
   ],
   "source": [
    "global_list = []\n",
    "\n",
    "def log2(x, k):\n",
    "  global_list.append(x)\n",
    "  ln_x = jnp.log(x)\n",
    "  ln_2 = jnp.log(k)\n",
    "  return ln_x / ln_2\n",
    "\n",
    "print(jax.make_jaxpr(log2)(3.0, 5.)) \n",
    "#Jaxpr: Low-level, compiled thing. You don't run it. It just exists (kind of like backend stuff thats still cool)\n",
    "#jaxpr made by running it on the args u give while also tracing stuff."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f677f51-24d0-47e1-ae50-8ff7837b1ee5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# IMPORTANT: Does not capture anything about global_list.append(x)\n",
    "- Feature, not a bug. Basically wants fxns that depend only on their args\n",
    "- Impure fxns (fxns that read/write to a global state are bad due to the compiler doing weird things).\n",
    "- Ie it can cache the global state as 4.0, then the global gets updated to 5, but the cached value stays 4 and then your computations go no no.\n",
    "- Print is included as impure.\n",
    "- Basically just pass anything that the fxn depends on as an arg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce78592e-def0-4d27-a3b8-8e0f09f0f066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length 2 and list [Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace>, Traced<ShapedArray(int32[], weak_type=True)>with<DynamicJaxprTrace>] before\n",
      "length 2 and list [Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace>, Traced<ShapedArray(int32[], weak_type=True)>with<DynamicJaxprTrace>] after\n"
     ]
    }
   ],
   "source": [
    "jitted_log2 = jax.jit(log2)\n",
    "jitted_log2(3, 5)\n",
    "print(f\"length {len(global_list)} and list {global_list} before\")\n",
    "jitted_log2(1, 5)\n",
    "#Should make global_list bigger but it DOESNT due to thing above.\n",
    "print(f\"length {len(global_list)} and list {global_list} after\")\n",
    "\n",
    "#Side note: The first time does make it run the append but that might be just due the first call including a tracing pass (which includes the global append) but subsequent ones don't\n",
    "#IN OTHER WORDS DONT READ/WRITE GLOBALS. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1bdf0b-fd69-4617-b556-395d2b6667d9",
   "metadata": {},
   "source": [
    "# Conditionals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b0eba368-1ebc-46cf-83f5-02fd11332dd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(30, dtype=int32, weak_type=True)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#so jax gets very not happy with conditionals.# While loop conditioned on x and n with a jitted body.\n",
    "#Avoid that!\n",
    "\n",
    "@jax.jit\n",
    "def loop_body(prev_i):\n",
    "  return prev_i + 1\n",
    "\n",
    "def g_inner_jitted(x, n):\n",
    "  i = 0\n",
    "  while i < n:\n",
    "    i = loop_body(i) #Here, computationally expensive loop body is jitted but the while loop\n",
    "      #is not jitted so jax doesnt have to compile a fxn conditional on something that is known only at runtime (n)\n",
    "  return x + i\n",
    "\n",
    "g_inner_jitted(10, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "22c00eeb-6be3-48dc-b820-5f9df52d4a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Or option 2: use staticargnames. This will work, but is not great as it has to recompile for each new value of the nums in static argnums\n",
    "#Static means python values not jax arrays\n",
    "\n",
    "jit_cond = jax.jit(g_inner_jitted, static_argnames='n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "18ec044a-e0d5-48db-9cd5-a680c3c4389f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([1.], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Best:\n",
    "\n",
    "#jnp.where, jnp.piecewise -> function like np.piecwise fxns\n",
    "\n",
    "#Jax.lax.cond is where true_fun is ran if its true, false if its false and pred is the condtional the if(conditional)\n",
    "#compiles both branches and evaulates the if at runtime\n",
    "def cond(pred, true_fun, false_fun, operand):\n",
    "  if pred:\n",
    "    return true_fun(operand)\n",
    "  else:\n",
    "    return false_fun(operand)\n",
    "\n",
    "operand = jnp.array([0.])\n",
    "lax.cond(True, lambda x: x+1, lambda x: x-1, operand)\n",
    "\n",
    "#Also: fori and while loops: Jax does cool stuff using XLA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0495fb1f-1fde-4afb-a876-430f870ae538",
   "metadata": {},
   "source": [
    "# Autovectorization:\n",
    "jax.vmap() adds a batch ax to beginning of each input. Can also use in_axes or out_axes to specify the location of batch dimensions\n",
    "\n",
    "batch_convolve_v3 = jax.vmap(convolve, in_axes=[0, None])\n",
    "\n",
    "batch_convolve_v3(xs, w)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31b9722-2ce0-4e0b-a2da-b88aa31c8300",
   "metadata": {},
   "source": [
    "# Autodiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "85d6363d-b396-4bfc-8f5a-aaa60d452b8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.07065082, dtype=float32, weak_type=True)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_tanh = grad(jnp.tanh) #grad_fxn = grad(fxn_name). Returns a fxn \n",
    "gradded_val = grad(jnp.tanh)(2.)\n",
    "gradded_val\n",
    "\n",
    "# loss_value, Wb_grad = jax.value_and_grad(fxn, (0, 1))(W, b)\n",
    "#Also: argnums is either int or list of ints (positional) to differentiate wrt. \n",
    "#Returns same datastructure/type as the argnums"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a237d8f-e410-4597-9872-c29d82c7453f",
   "metadata": {},
   "source": [
    "# Pytree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8276e421-e112-454c-94b8-95b80e652f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basically nested data structures can be seen as a tree. Can make some of these but idt we need to worry about that rn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a7e7a7-f3d3-4baf-9afb-9016b12fac84",
   "metadata": {},
   "source": [
    "# Sharp bits\n",
    "jax can only differentiate immutables (that are immutable in the same way a str is. Can += but just reassings)  \n",
    "Uses x.at[idx].set(y) , .add(num), .multiply(num), etc. These are addings that are made rlly fast under hood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20311c6-ff30-4f43-a518-40a8060d6586",
   "metadata": {},
   "source": [
    "# I try einsum and other np functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3cc4944c-92a1-4ffc-afe1-eaeb16d18da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f34409-2280-4926-9afb-c360e63c8a8c",
   "metadata": {},
   "source": [
    "So it goes through the test1 in column major order (columns or the right side changing fastest) and then shoves it into the arr of the new shape in column major (w/ columns, or the right side changing fastest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8720eec0-971a-4431-a498-a1dc292f0334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0  1  2  3  4]\n",
      "  [ 5  6  7  8  9]]\n",
      "\n",
      " [[10 11 12 13 14]\n",
      "  [15 16 17 18 19]]]\n"
     ]
    }
   ],
   "source": [
    "test1 = np.arange(20).reshape((2,2, 5), order='c')\n",
    "print(test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fcfbad77-5c06-4d0e-905d-c057d6ec7c34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 0,  1,  2],\n",
       "         [ 3,  4,  5],\n",
       "         [ 6,  7,  8]],\n",
       "\n",
       "        [[ 9, 10, 11],\n",
       "         [12, 13, 14],\n",
       "         [15, 16, 17]]],\n",
       "\n",
       "\n",
       "       [[[18, 19, 20],\n",
       "         [21, 22, 23],\n",
       "         [24, 25, 26]],\n",
       "\n",
       "        [[27, 28, 29],\n",
       "         [30, 31, 32],\n",
       "         [33, 34, 35]]]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now lets say we got something of bhwc:\n",
    "batched_img = np.arange(36).reshape(2,2,3,3) #Batch of 2 images w/ 2 rows and 3 columns and 3 channels. So each of those 3 channels = 1 pixel normally\n",
    "batched_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c90c794e-3ca6-4c92-afd8-be41f77305a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0,  1,  2],\n",
       "        [ 3,  4,  5],\n",
       "        [ 6,  7,  8],\n",
       "        [ 9, 10, 11],\n",
       "        [12, 13, 14],\n",
       "        [15, 16, 17]],\n",
       "\n",
       "       [[18, 19, 20],\n",
       "        [21, 22, 23],\n",
       "        [24, 25, 26],\n",
       "        [27, 28, 29],\n",
       "        [30, 31, 32],\n",
       "        [33, 34, 35]]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#if I wanted to flatten it but retain channels:\n",
    "test2 = batched_img.reshape(2,6,3) #functions because each 3 values in the flattened get read into the same pixel.\n",
    "test2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f7da7b-3400-4081-b385-0f44f12c0b13",
   "metadata": {},
   "source": [
    "All right now onto the hard part: Einstein summation. Even the name is intimidating.  \n",
    "np.einsum('in1.shape,in2.shape->output_shape', in1, in2). Also can omit stuff from output_shape to tell it to sum over those axes  \n",
    "Examples are the only way I can explain this so here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b1ddc41d-d237-4833-82f8-3fecf2a8b5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Matrix mult.\n",
    "A = np.random.rand(2, 3)\n",
    "B = np.random.rand(3, 4)\n",
    "C = np.einsum('ik,kj->ij', A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f31c6596-5b7f-42bd-8324-76c42792ed47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5] [0 1 2 3 4 5]\n",
      "55 225 [[ 0  0  0  0  0  0]\n",
      " [ 0  1  2  3  4  5]\n",
      " [ 0  2  4  6  8 10]\n",
      " [ 0  3  6  9 12 15]\n",
      " [ 0  4  8 12 16 20]\n",
      " [ 0  5 10 15 20 25]]\n"
     ]
    }
   ],
   "source": [
    "#Dot product\n",
    "A = np.arange(6)\n",
    "B = np.arange(6)\n",
    "dot_prod = np.einsum('i,i->', A, B) #holy shit theres something wacko going on. think its cause they sum it after computing the outer product in the second scanrio\n",
    "outer_prod_sum = np.einsum('i,b->', A, B)\n",
    "outer_prod = np.einsum('i,b->ib', A, B)\n",
    "print(A, B)\n",
    "print(dot_prod, outer_prod_sum, outer_prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "08c30380-7262-4666-ac21-c2b39031b479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2]\n",
      " [3 4 5]]\n",
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "[ 8 62]\n"
     ]
    }
   ],
   "source": [
    "#Tensordot over specific axes:\n",
    "A1 = np.arange(6).reshape(2, 3)\n",
    "B1 = np.arange(6).reshape(2,3) + 1\n",
    "#I want to run tensor over ax 1 while leaving ax 0 unchanged. So result be of size (2, 1)\n",
    "res = np.einsum('ab,ab->a', A1, B1)\n",
    "print(A1)\n",
    "print(B1)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ce657292-03c0-4ee9-a1d4-18d430507a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 0  1  2]\n",
      "   [ 3  4  5]]\n",
      "\n",
      "  [[ 6  7  8]\n",
      "   [ 9 10 11]]]]\n",
      "-\n",
      "[[[ 0  1  2  3  4]\n",
      "  [ 5  6  7  8  9]]\n",
      "\n",
      " [[10 11 12 13 14]\n",
      "  [15 16 17 18 19]]]\n",
      "slow\n",
      "[[720. 786. 852. 918. 984.]]\n",
      "einsummed\n",
      "[[720 786 852 918 984]]\n"
     ]
    }
   ],
   "source": [
    "#Now we get relevant to CNNs again. \n",
    "A2 = np.arange(12).reshape(1,2,2,3) #Slice of 5-batch 3 channel image\n",
    "k = np.arange(20).reshape(2,2,5) #2x2kernel w/ 5 outputfilters\n",
    "\n",
    "convolved = np.einsum('abxc,bxd->ad',A2, k) #oh so this works but \n",
    "crap_convolved = np.einsum('abbc,bbd->ad',A2, k) #doesnt work. According to google it performed a summation over the diagonal or smth\n",
    "\n",
    "#I aint manaully checking this so lets write a nested for loop\n",
    "ret_slow = np.zeros((A2.shape[0], k.shape[2]))\n",
    "for b in range(A2.shape[0]): #we manually loop over every batch ughh this is hurting me already\n",
    "    #We manually loop over the filters\n",
    "    for f in range(k.shape[2]):\n",
    "        ret_slow[b,f] = np.sum( A2[b] * k[:,:,f, np.newaxis])\n",
    "        \n",
    "print(A2)\n",
    "print(\"-\")\n",
    "print(k)\n",
    "print(\"slow\")\n",
    "print(ret_slow)\n",
    "print(\"einsummed\")\n",
    "print(convolved)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03b9741-0fd8-4adb-8abe-35418cc3d1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Why does einsum tweak out with duplicate axes? Time to find out!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
