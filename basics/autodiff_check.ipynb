{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ada22e5-bc2e-4e08-b42e-e87ce41f8d49",
   "metadata": {},
   "source": [
    "# Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3b82e03-6a72-4875-8fb5-9fadb0360f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import jax.numpy as jnp\n",
    "from jax import grad\n",
    "import jax\n",
    "import numpy as np\n",
    "import math\n",
    "%matplotlib inline\n",
    "from grad import *\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ee48bf",
   "metadata": {},
   "source": [
    "# Test of my autodiff library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab9ded4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = Number(1)\n",
    "x2 = Number(2)\n",
    "x3 = Number(3)\n",
    "x4 = Number(4)\n",
    "x5 = Number(5)\n",
    "\n",
    "y = (x1+x2)*x5 - x3\n",
    "top_sorted = topo_sort(y)\n",
    "print(top_sorted)\n",
    "\n",
    "y.null_gradients()\n",
    "\n",
    "for num in top_sorted:\n",
    "    num.backprop_single()\n",
    "    \n",
    "print([(i.grad, i) for i in top_sorted])\n",
    "\n",
    "y.null_gradients()\n",
    "y.backprop()\n",
    "\n",
    "print( [(i.grad, i) for i in top_sorted], \"key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "26de7cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Number(0.8807970779778823), Number(1.1353352832366128), Number(0.1353352832366127), Number(-2.0), Number(2.0), Number(-1), Number(2.718281828459045), Number(1), Number(1)]\n",
      "value comparison: Mine Number(0.8807970779778823) Jax 0.8807970285415649\n",
      "Grad comparison: Mine [0.1049935854035065] Jax[0.10499357]\n"
     ]
    }
   ],
   "source": [
    "testmine = Number(2.)\n",
    "mysigmoid = 1/(1+math.e**-testmine)\n",
    "\n",
    "mysigmoid.backprop(should_print=False)\n",
    "print(topo_sort(mysigmoid))\n",
    "\n",
    "def jaxsigmoidsum(x):\n",
    "    x = jnp.sum(x)\n",
    "    return 1 / (1 + jnp.exp(-x)) \n",
    "\n",
    "testjax = jnp.array([2.]) \n",
    "sigmoided_value, grads = jax.value_and_grad(jaxsigmoidsum, argnums=(0))(testjax)\n",
    "\n",
    "print(f\"value comparison:\", f\"Mine {mysigmoid}\", f\"Jax {sigmoided_value}\")\n",
    "print(f\"Grad comparison:\", f\"Mine {[testmine.grad]}\", f\"Jax{grads}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc804d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jax_sigmoid(x):\n",
    "    return jnp.vectorize(lambda x: 1/(1+math.e**-x))(x)\n",
    "\n",
    "def jax_weight_matrix(shape, naive=False):\n",
    "    \"\"\"weight matrix thingy.give dims. Not 0.\"\"\"\n",
    "    number = 1\n",
    "    if(type(shape) == int):\n",
    "        shape = [shape]\n",
    "    for i in shape:\n",
    "        number*= i\n",
    "    if naive:\n",
    "        return jnp.array([(i / 10) for i in range(number)]).reshape(*shape)\n",
    "    return np.array([np.random.uniform(low=-.2, high=.2, size=None) for i in range(number)]).reshape(*shape)\n",
    "    # return np.array([variable(np.random.uniform(low=-.2, high=.2, size=None)) for i in range(sizes[0] * sizes[1])).reshape(*shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ec10c353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number(10.65)\n",
      "10.65\n",
      "[1.5       1.5       1.8000001 1.8000001 2.1       2.1       2.4\n",
      " 2.4       2.6999998 2.6999998]\n",
      "[1.5, 1.5, 1.8, 1.8, 2.0999999999999996, 2.0999999999999996, 2.4000000000000004, 2.4000000000000004, 2.7, 2.7]\n"
     ]
    }
   ],
   "source": [
    "test_shape = (3,5)\n",
    "test_jax = jax_weight_matrix(test_shape, naive=True)\n",
    "test_mine = weight_matrix(test_shape, naive=True)\n",
    "test_shape2 = (5, 2)\n",
    "test_jax2 = jax_weight_matrix(test_shape2, naive=True)\n",
    "test_mine2 = weight_matrix(test_shape2, naive=True)\n",
    "\n",
    "my_matmul = np.sum(test_mine @ test_mine2)\n",
    "def j_matmul(a, b):\n",
    "    thing = a @ b\n",
    "    return jnp.sum(thing)\n",
    "\n",
    "\n",
    "print(my_matmul)\n",
    "print(j_matmul(test_jax, test_jax2))\n",
    "\n",
    "j_matmuled, grads = jax.value_and_grad(j_matmul, argnums=(0, 1))(test_jax, test_jax2)\n",
    "\n",
    "my_matmul.backprop()\n",
    "\n",
    "print(grads[1].flatten())\n",
    "print([thing.grad for thing in test_mine2.flat])\n",
    "#These match almost exactly! Yay!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe71d89b-c6fb-4a9d-8eaa-f15e763836c4",
   "metadata": {},
   "source": [
    "# Overfitting a single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdd3817b-2859-4488-aa24-952606bae576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e3c7d2b-44bf-429b-818b-33a400c56644",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data(path=\"mnist.npz\", )\n",
    "indices = np.arange(len(x_train))\n",
    "np.random.shuffle(indices)\n",
    "x_train = x_train[indices]\n",
    "y_train = y_train[indices]\n",
    "\n",
    "def batch(x, y, batch_size=32):\n",
    "    if len(x) % batch_size != 0:\n",
    "        x = x[:batch_size * (len(x)//batch_size)]\n",
    "        y=y[:batch_size*(len(x)//batch_size)]\n",
    "    return np.array_split(x, len(x) / batch_size, axis=0), np.array_split(y, len(y)/batch_size, axis=0)\n",
    "\n",
    "def fix_data(x, y):\n",
    "    x = x.reshape(x.shape[0], 28*28)/255\n",
    "    test = np.zeros((x.shape[0], 10))\n",
    "    test[np.arange(x.shape[0]),y] = 1\n",
    "    return (x, test)\n",
    "\n",
    "fixed_x, fixed_y = fix_data(x_train[:1000], y_train[:1000])\n",
    "b_x , b_y = batch(fixed_x, fixed_y, 32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cdfd579-df78-4222-b44d-d6d4b477ecce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting epoch 0\n",
      "[[Number(5.1010880362035633e-08) Number(9.338977984736345e-09)\n",
      "  Number(2.8903566673143343e-06) Number(5.126523416196103e-08)\n",
      "  Number(2.3552644732446952e-07) Number(2.027152204304169e-05)\n",
      "  Number(3.384781919372198e-08) Number(6.918581099002038e-10)\n",
      "  Number(5.1484685790866545e-09) Number(1.5024589695836614e-07)]\n",
      " [Number(0.0002684087058041381) Number(0.00010845026752175577)\n",
      "  Number(0.0015633470709564435) Number(0.00020293521255668952)\n",
      "  Number(0.00036582101973399185) Number(0.0028623524842888164)\n",
      "  Number(0.0001443077506618142) Number(4.3047795436393466e-05)\n",
      "  Number(0.00011214943173866973) Number(0.0006296488037372414)]\n",
      " [Number(1.918242019791202e-06) Number(4.772063647064944e-07)\n",
      "  Number(4.048811685754022e-05) Number(1.7073169609073928e-06)\n",
      "  Number(5.208912030645319e-06) Number(0.00015495049033421122)\n",
      "  Number(1.1436706530865938e-06) Number(7.167557464409456e-08)\n",
      "  Number(3.626945727450202e-07) Number(5.395131553999207e-06)]\n",
      " [Number(3.766422478893002e-05) Number(1.2908908567927905e-05)\n",
      "  Number(0.00037351723638900507) Number(3.044203535561301e-05)\n",
      "  Number(6.788660538620343e-05) Number(0.0009516296249042535)\n",
      "  Number(2.129616239289072e-05) Number(3.4585739078454597e-06)\n",
      "  Number(1.1179979715344097e-05) Number(9.026027656113999e-05)]\n",
      " [Number(5.164181830471462e-06) Number(1.3690610776405008e-06)\n",
      "  Number(8.51287003670389e-05) Number(4.472230047592203e-06)\n",
      "  Number(1.2757898024617916e-05) Number(0.0002638794324852097)\n",
      "  Number(3.1562934922034525e-06) Number(2.666330652873616e-07)\n",
      "  Number(1.1911526781654357e-06) Number(1.5229232404046226e-05)]\n",
      " [Number(3.423183833067472e-05) Number(1.2108684436777705e-05)\n",
      "  Number(0.0003497526654233473) Number(2.7609326066192746e-05)\n",
      "  Number(6.09111469947339e-05) Number(0.000964293625233789)\n",
      "  Number(1.8963777756659502e-05) Number(3.0210346118168322e-06)\n",
      "  Number(9.543446298767569e-06) Number(7.516694685310051e-05)]\n",
      " [Number(3.822137551308364e-05) Number(1.2335017640470862e-05)\n",
      "  Number(0.00037160497788983306) Number(3.165407921634748e-05)\n",
      "  Number(7.011733921562048e-05) Number(0.0008876150352848496)\n",
      "  Number(2.1305600810257285e-05) Number(3.6304662802371145e-06)\n",
      "  Number(1.2479296754587961e-05) Number(0.00010432420218258967)]\n",
      " [Number(2.1087463857526915e-06) Number(5.523135280617515e-07)\n",
      "  Number(4.565979593608427e-05) Number(1.877251673966551e-06)\n",
      "  Number(5.883408461029181e-06) Number(0.00018034850519528016)\n",
      "  Number(1.319501256113563e-06) Number(8.6543609938718e-08)\n",
      "  Number(3.899018798380551e-07) Number(5.537127411353278e-06)]\n",
      " [Number(1.5626700710033208e-06) Number(3.7754143533893054e-07)\n",
      "  Number(3.4562703967998294e-05) Number(1.4075835965787275e-06)\n",
      "  Number(4.34838491114683e-06) Number(0.00013592144765016176)\n",
      "  Number(9.234620878741006e-07) Number(5.499986992468098e-08)\n",
      "  Number(2.904492994403319e-07) Number(4.502727218364406e-06)]\n",
      " [Number(4.717087367860365e-05) Number(1.610026845971023e-05)\n",
      "  Number(0.00043419315060155535) Number(3.783377981306481e-05)\n",
      "  Number(8.222894227461593e-05) Number(0.0010304346871750675)\n",
      "  Number(2.642761134410884e-05) Number(4.551700464599045e-06)\n",
      "  Number(1.4895595518734864e-05) Number(0.00011787120679315508)]\n",
      " [Number(1.2045342780864585e-05) Number(3.648210688714486e-06)\n",
      "  Number(0.0001613715591818864) Number(1.0117372095205222e-05)\n",
      "  Number(2.5864536645860862e-05) Number(0.000476663122136739)\n",
      "  Number(7.095993412739784e-06) Number(7.991829764848662e-07)\n",
      "  Number(3.0170363132029592e-06) Number(3.097813519349755e-05)]\n",
      " [Number(0.0003171937503398309) Number(0.00013153749621778828)\n",
      "  Number(0.0017554033604682921) Number(0.00023613666394800236)\n",
      "  Number(0.0004182237552901781) Number(0.003221023269166375)\n",
      "  Number(0.00016730597405981528) Number(5.3008412874358574e-05)\n",
      "  Number(0.00013435945625180952) Number(0.0007180980341931723)]\n",
      " [Number(2.7040505283258406e-06) Number(7.269789488708273e-07)\n",
      "  Number(5.533515028436084e-05) Number(2.38374848847544e-06)\n",
      "  Number(7.372667594722489e-06) Number(0.00020943121297875472)\n",
      "  Number(1.7118745751436475e-06) Number(1.2046924702898307e-07)\n",
      "  Number(5.187678082539699e-07) Number(7.0429117789250744e-06)]\n",
      " [Number(4.840694960635045e-07) Number(1.1436061361514734e-07)\n",
      "  Number(1.5272054462533356e-05) Number(4.6359532837045737e-07)\n",
      "  Number(1.57458310365236e-06) Number(8.1669448236164e-05)\n",
      "  Number(2.941456919096167e-07) Number(1.269834605179905e-08)\n",
      "  Number(6.956633122461579e-08) Number(1.2535992054455522e-06)]\n",
      " [Number(2.661926948582092e-06) Number(6.556308636186314e-07)\n",
      "  Number(5.160065503956023e-05) Number(2.350450268309418e-06)\n",
      "  Number(7.156071223432743e-06) Number(0.00017456945580207234)\n",
      "  Number(1.6366052977146163e-06) Number(1.1125164291568449e-07)\n",
      "  Number(5.552135724282556e-07) Number(8.147856532614288e-06)]\n",
      " [Number(8.776657586608034e-05) Number(3.116820600333744e-05)\n",
      "  Number(0.0006972513057857312) Number(7.180261628438544e-05)\n",
      "  Number(0.00014506446499943303) Number(0.0014574360249638152)\n",
      "  Number(5.047456397854977e-05) Number(1.0644443427171928e-05)\n",
      "  Number(3.3159400861365287e-05) Number(0.00024074724586194647)]\n",
      " [Number(5.351374705327658e-07) Number(1.1224144834326699e-07)\n",
      "  Number(1.6336305857325423e-05) Number(4.992471555047913e-07)\n",
      "  Number(1.9083905792843443e-06) Number(6.823516431253603e-05)\n",
      "  Number(3.647510148051532e-07) Number(1.4766145560050788e-08)\n",
      "  Number(8.692614438339377e-08) Number(1.7882799533308614e-06)]\n",
      " [Number(1.93646927861549e-05) Number(5.859963648324362e-06)\n",
      "  Number(0.00022727566055780245) Number(1.6279862796615526e-05)\n",
      "  Number(3.9768789913179574e-05) Number(0.0005916900464085999)\n",
      "  Number(1.1309025133475455e-05) Number(1.5135424965083794e-06)\n",
      "  Number(5.603502014923998e-06) Number(5.425921391871298e-05)]\n",
      " [Number(1.1952035867494066e-05) Number(3.4671172039224994e-06)\n",
      "  Number(0.00015886905060164265) Number(1.0065417872923138e-05)\n",
      "  Number(2.6222335239760672e-05) Number(0.0004391526222616505)\n",
      "  Number(7.174842600181309e-06) Number(7.915440454772007e-07)\n",
      "  Number(3.128265927241468e-06) Number(3.366208250555667e-05)]\n",
      " [Number(6.172169408633272e-06) Number(1.717288630537732e-06)\n",
      "  Number(9.792569652691404e-05) Number(5.306550949956661e-06)\n",
      "  Number(1.4674326307259532e-05) Number(0.0003103459132694327)\n",
      "  Number(3.7127626943425967e-06) Number(3.363569372965773e-07)\n",
      "  Number(1.4181439251222828e-06) Number(1.6941720405493956e-05)]\n",
      " [Number(1.134938933229411e-06) Number(2.725618399276377e-07)\n",
      "  Number(2.824710421232318e-05) Number(1.030768035432833e-06)\n",
      "  Number(3.4131977817055574e-06) Number(0.00011854504828044153)\n",
      "  Number(7.076465752123451e-07) Number(3.787011993902836e-08)\n",
      "  Number(1.9482822167522236e-07) Number(3.190429844795255e-06)]\n",
      " [Number(3.101776053530822e-06) Number(8.677679723386233e-07)\n",
      "  Number(6.120304039200352e-05) Number(2.774082115180375e-06)\n",
      "  Number(8.052567792060647e-06) Number(0.00024558240186335243)\n",
      "  Number(1.8439725585059226e-06) Number(1.46712274472111e-07)\n",
      "  Number(6.06486116895849e-07) Number(7.572246547089134e-06)]\n",
      " [Number(6.4221346874311305e-06) Number(1.76840225607343e-06)\n",
      "  Number(9.934766836436533e-05) Number(5.505100730664703e-06)\n",
      "  Number(1.502773488875439e-05) Number(0.00030743980303240794)\n",
      "  Number(3.8132834003336458e-06) Number(3.469308170479781e-07)\n",
      "  Number(1.5031201246091662e-06) Number(1.80177905720339e-05)]\n",
      " [Number(5.673500347426661e-07) Number(1.1988350296653628e-07)\n",
      "  Number(1.7208770893262615e-05) Number(5.291865114760412e-07)\n",
      "  Number(2.0303492397704847e-06) Number(7.11946078406149e-05)\n",
      "  Number(3.9115403971805783e-07) Number(1.6168139772098577e-08)\n",
      "  Number(9.305855485400131e-08) Number(1.8950111890306494e-06)]\n",
      " [Number(2.326939540961881e-07) Number(4.6801899977950156e-08)\n",
      "  Number(8.670569198788431e-06) Number(2.2255317385467738e-07)\n",
      "  Number(8.776814350422754e-07) Number(4.4575114146069034e-05)\n",
      "  Number(1.5065770233100526e-07) Number(4.834830426481425e-09)\n",
      "  Number(3.165243479345184e-08) Number(7.242937254190141e-07)]\n",
      " [Number(3.123335983315688e-08) Number(4.914330294743659e-09)\n",
      "  Number(1.9260980036182186e-06) Number(3.190718440301084e-08)\n",
      "  Number(1.5790565883521452e-07) Number(1.240042889043561e-05)\n",
      "  Number(2.1317777007698243e-08) Number(3.542724185735894e-10)\n",
      "  Number(3.2311582551082552e-09) Number(1.1543450893199407e-07)]\n",
      " [Number(5.3064490313638864e-06) Number(1.5047502814296432e-06)\n",
      "  Number(9.106459571892106e-05) Number(4.519428117583662e-06)\n",
      "  Number(1.3337896262005592e-05) Number(0.00029934386854597276)\n",
      "  Number(3.4179268509255462e-06) Number(2.838677195381542e-07)\n",
      "  Number(1.1306599828682845e-06) Number(1.3852994472808092e-05)]\n",
      " [Number(1.8689798383778905e-08) Number(3.0699133028586936e-09)\n",
      "  Number(1.424976731841333e-06) Number(1.937576882015232e-08)\n",
      "  Number(1.0581399493703747e-07) Number(1.1077330692371868e-05)\n",
      "  Number(1.3732634581016791e-08) Number(1.9835137051043105e-10)\n",
      "  Number(1.621054906235475e-09) Number(5.964527114627729e-08)]\n",
      " [Number(1.5877419245765278e-06) Number(3.696716532677356e-07)\n",
      "  Number(3.4986444744618534e-05) Number(1.4254108983011307e-06)\n",
      "  Number(4.565111599722675e-06) Number(0.0001271251898793542)\n",
      "  Number(9.804474562933942e-07) Number(5.6530827083091913e-08)\n",
      "  Number(3.0636062972784964e-07) Number(4.99470402918607e-06)]\n",
      " [Number(1.1988914992709858e-07) Number(2.237767055951745e-08)\n",
      "  Number(5.29228716870363e-06) Number(1.1727154447147042e-07)\n",
      "  Number(4.976774915425315e-07) Number(2.9621699517358443e-05)\n",
      "  Number(7.886479651157532e-08) Number(2.0478010046032814e-09)\n",
      "  Number(1.4816876052634723e-08) Number(3.8996792146086265e-07)]\n",
      " [Number(9.133112149873808e-07) Number(2.0879106212029852e-07)\n",
      "  Number(2.528650557180425e-05) Number(8.310856139337696e-07)\n",
      "  Number(3.13537911499767e-06) Number(9.96243240277869e-05)\n",
      "  Number(6.528066437730574e-07) Number(3.068311946366398e-08)\n",
      "  Number(1.5499874322531446e-07) Number(2.8392440841000305e-06)]\n",
      " [Number(0.00012262293158909774) Number(4.4846698652115616e-05)\n",
      "  Number(0.0008834124894334446) Number(9.860074306150614e-05)\n",
      "  Number(0.00018976120545228234) Number(0.0018485128819530554)\n",
      "  Number(6.520573186518182e-05) Number(1.6751510074404632e-05)\n",
      "  Number(4.8133217834823816e-05) Number(0.000310842086502891)]]\n",
      "Acc: 0.07560483870967742 Avg loss: 33.72865656334712\n",
      "Elapsed time for one epoch: 139.63887100003194 seconds\n",
      "starting epoch 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m10\u001b[39m):\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mstarting epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     datas.append(\u001b[43mmy_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimer\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_timer\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m#as you can see loss does go down and it manages to predict the single image.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jseto\\Downloads\\scritch_scratch\\basics\\model.py:162\u001b[39m, in \u001b[36mModel.train_epoch\u001b[39m\u001b[34m(self, x, y, lr, timer, batch_timer)\u001b[39m\n\u001b[32m    159\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mElapsed time for fd pass: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;250m \u001b[39mtime.perf_counter()\u001b[38;5;250m  \u001b[39m-\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m seconds\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    160\u001b[39m     start_time = time.perf_counter() \n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m y = np.array(y)\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers:\n\u001b[32m    165\u001b[39m      \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m layer.flat:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "my_model = Model(28*28, 10, [ 8, 16])\n",
    "datas = []\n",
    "for _epoch in range(10):\n",
    "    print(f\"starting epoch {_epoch}\")\n",
    "    datas.append(my_model.train_epoch(b_x, b_y, lr=1, timer=False, batch_timer=False))\n",
    "#as you can see loss does go down and it manages to predict the single image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18db4e7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f4c324",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to display img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b04aeb",
   "metadata": {},
   "source": [
    "# Attempt to actually train it on multiple images\n",
    "\n",
    "For reasons that are embedded in my terrible architecture decisions, this is very long. (These reasons include the topo sort not being cached, which would be fairly difficult to implement due to how Numbers() are created. I did not make this homemade autodiff library for speed or even to truly train something; I made it to understand autodiff, which I think it has suceeded in doing, as demonstrated by above cells.)\n",
    "\n",
    "Running any of the cells below may result in it taking over half an hour to an hour to truly train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3ca25994-ac22-460b-a803-cdd346687cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312.0\n"
     ]
    }
   ],
   "source": [
    "full_x, full_y = fix_data(x_train[:10000], y_train[:10000])\n",
    "full_b_x , full_b_y = batch(fixed_x, fixed_y)\n",
    "\n",
    "my_model = Model(28*28, 10, [ 8, 16])\n",
    "datas = []\n",
    "for _epoch in range(30):\n",
    "    print(f\"starting epoch {_epoch}\")\n",
    "    datas.append(my_model.train_epoch(b_x, b_y, lr=1e-2, timer=False, batch_timer=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b0d5a9-29d0-4a33-8ff4-145ca4db75a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline  \n",
    "\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "pic = 1\n",
    "for i, img in enumerate(x_test[:10]):\n",
    "  plt.subplot(2, 5, pic)\n",
    "  plt.axis('off')\n",
    "  predicted = my_model.fd(img.flat)\n",
    "  plt.title(f\"T {y_test[i]} mine {np.argmax(predicted)} \")\n",
    "  plt.imshow(img)\n",
    "  pic+= 1\n",
    "plt.show()\n",
    "#60% acc. Considering this is from nearly scratch not terrible "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
