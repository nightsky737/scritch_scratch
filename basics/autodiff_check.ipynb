{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ada22e5-bc2e-4e08-b42e-e87ce41f8d49",
   "metadata": {},
   "source": [
    "# To Do:\n",
    "#i have a terrible idea: but wait wtf i dont tuple it later idk why its not working\n",
    "Figure out Jax\n",
    "Figure out fun with adagrad/etc\n",
    "Figure out cnn\n",
    "AND THEN WHAT I DID THIS ALL FOR! GRADIENT ASCENT BABY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3b82e03-6a72-4875-8fb5-9fadb0360f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import jax.numpy as jnp\n",
    "from jax import grad\n",
    "import jax\n",
    "import numpy as np\n",
    "import math\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a84d4012-a431-4d5c-af5e-23d31c2c3ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_matrix(shape, naive=False):\n",
    "    \"\"\"weight matrix thingy.give dims. Not 0.\"\"\"\n",
    "    number = 1\n",
    "    if(type(shape) == int):\n",
    "        shape = [shape]\n",
    "    for i in shape:\n",
    "        number*= i\n",
    "    if naive:\n",
    "        return jnp.array([(i / 10) for i in range(number)]).reshape(*shape)\n",
    "    return np.array([np.random.uniform(low=-.2, high=.2, size=None) for i in range(number)]).reshape(*shape)\n",
    "    # return np.array([variable(np.random.uniform(low=-.2, high=.2, size=None)) for i in range(sizes[0] * sizes[1])).reshape(*shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3edd1fd6-dd9e-470e-8f01-4c2325d3f006",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def sigmoid(x):\n",
    "    return jnp.vectorize(lambda x: 1/(1+math.e**-x))(x)\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return jnp.where(x <= 0, 1e-2 * x, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b33f42b6-2ca4-46bc-bae6-9184d9b9d6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "class Model():\n",
    "    def __init__(self, input_size, output_size, hidden_layers, naive=False, seed=None):\n",
    "        '''\n",
    "        Takes list of # of things in their layers.\n",
    "        Layers are outputs?\n",
    "        '''\n",
    "        if seed != None:\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        self.layer_sizes = hidden_layers\n",
    "        self.layers = []\n",
    "        self.biases = []\n",
    "        \n",
    "        prev_size = input_size\n",
    "        \n",
    "        for hidden_layer in hidden_layers:\n",
    "            self.layers.append(weight_matrix([prev_size, hidden_layer], naive))\n",
    "            self.biases.append(weight_matrix(hidden_layer, naive))\n",
    "            prev_size  = hidden_layer\n",
    "            \n",
    "        self.biases.append(weight_matrix([output_size]))\n",
    "        self.layers.append(weight_matrix([prev_size, output_size]))\n",
    "\n",
    "        self.layers= tuple(self.layers)\n",
    "        self.biases = tuple(self.biases)  \n",
    " \n",
    "  \n",
    "    def fd(self, x):\n",
    "        '''f pass with input. '''\n",
    "\n",
    "        for i in range(len(self.layers)):\n",
    "            x = x @ self.layers[i]\n",
    "            x += self.biases[i]\n",
    "            if i != len(self.layers) - 1:\n",
    "                x = relu(x)\n",
    "            else:\n",
    "                x = sigmoid(x)\n",
    "            # self.hidden_states_activation.append(x)\n",
    "\n",
    "        return x\n",
    "        \n",
    "    def loss_static(self, params, x, y):\n",
    "        '''f pass with for loss.  '''\n",
    "        w, b = params\n",
    "        for i in range(len(b)):\n",
    "            x = x @ w[i]\n",
    "            x += b[i]\n",
    "            if i != len(b) - 1:\n",
    "                x = relu(x)\n",
    "            else:\n",
    "                x = sigmoid(x)\n",
    "\n",
    "        y = jnp.array(y)\n",
    "        return jnp.sum(x * x - 2 * x * y + y * y)\n",
    "\n",
    "            \n",
    "    def train_epoch(self, x, y, lr=10**-2):\n",
    "        '''\n",
    "        f pass and then uh gradient descent?\n",
    "\n",
    "        x:  \n",
    "        y: the goal. In not sparse tensor.\n",
    "        lr: how quick it learns\n",
    "        '''\n",
    "        losses = []\n",
    "        x = np.array(x)\n",
    "        \n",
    "        for batch_num in range(len(y)):\n",
    "            mse, grads = jax.value_and_grad(self.loss_static, argnums=(0))((self.layers, self.biases), x[batch_num], y[batch_num])\n",
    "            \n",
    "            losses.append(mse)\n",
    "            \n",
    "           #0 contains weights and 1 contains the bias grads. \n",
    "            # print([i.shape for i in grads[1]])\n",
    "            # print([i.shape for i in grads[0]])\n",
    "\n",
    "            self.layers = list(self.layers)\n",
    "            self.biases = list(self.biases)\n",
    "            \n",
    "            for i, layer in enumerate(self.layers):\n",
    "                print(f\"Layer {i} avg grad:\", jnp.mean(grads[0][i]))\n",
    "\n",
    "\n",
    "            for i, (layer, grad_layer) in enumerate(zip(self.layers, grads[0])):\n",
    "                self.layers[i] = layer - lr * grad_layer  \n",
    "                \n",
    "            for i, (bias, grad_bias) in enumerate(zip(self.biases, grads[1])):\n",
    "                self.biases[i] = bias - lr * grad_bias   \n",
    "                \n",
    "        preds = self.fd(x[batch_num]) \n",
    "        \n",
    "        correct = jnp.sum(jnp.argmax(preds, axis=1) == jnp.argmax(y[batch_num], axis=1))\n",
    "        acc = correct / len(y[batch_num])\n",
    "        print(f\"Acc: {acc} Loss: {mse}\")\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe71d89b-c6fb-4a9d-8eaa-f15e763836c4",
   "metadata": {},
   "source": [
    "# if this doesnt work ima cry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdd3817b-2859-4488-aa24-952606bae576",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e3c7d2b-44bf-429b-818b-33a400c56644",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data(path=\"mnist.npz\", )\n",
    "indices = np.arange(len(x_train))\n",
    "np.random.shuffle(indices)\n",
    "x_train = x_train[indices]\n",
    "y_train = y_train[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cdfd579-df78-4222-b44d-d6d4b477ecce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(x, y, batch_size=32):\n",
    "    if len(x) % batch_size != 0:\n",
    "        x = x[:batch_size * (len(x)//batch_size)]\n",
    "        y=y[:batch_size*(len(x)//batch_size)]\n",
    "    print(len(x)/batch_size)\n",
    "    return np.array(np.split(x, int(len(x) / batch_size), axis=0)), np.split(y, int(len(y)/batch_size), axis=0)\n",
    "\n",
    "def fix_data(x, y):\n",
    "    x = x.reshape(x.shape[0], 28*28)/255\n",
    "    test = np.zeros((x.shape[0], 10))\n",
    "    test[np.arange(x.shape[0]),y] = 1\n",
    "    return (x, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ca25994-ac22-460b-a803-cdd346687cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312.0\n"
     ]
    }
   ],
   "source": [
    "fixed_x, fixed_y = fix_data(x_train[:10000], y_train[:10000])\n",
    "b_x , b_y = batch(fixed_x, fixed_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fb0e491d-46c3-4740-a153-657c959cfe34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 avg grad: -0.015709033\n",
      "Layer 1 avg grad: -0.17767176\n",
      "Layer 2 avg grad: 0.6838916\n",
      "Layer 0 avg grad: -0.0153417885\n",
      "Layer 1 avg grad: -0.2015289\n",
      "Layer 2 avg grad: 0.7275296\n",
      "Layer 0 avg grad: -0.00813491\n",
      "Layer 1 avg grad: -0.29927894\n",
      "Layer 2 avg grad: 0.7144414\n",
      "Layer 0 avg grad: -0.013045574\n",
      "Layer 1 avg grad: -0.2535523\n",
      "Layer 2 avg grad: 0.71853024\n",
      "Layer 0 avg grad: -0.0035471732\n",
      "Layer 1 avg grad: -0.18868366\n",
      "Layer 2 avg grad: 0.6662707\n",
      "Layer 0 avg grad: -0.0070924186\n",
      "Layer 1 avg grad: -0.22669001\n",
      "Layer 2 avg grad: 0.7357151\n",
      "Layer 0 avg grad: -0.016863491\n",
      "Layer 1 avg grad: -0.25603843\n",
      "Layer 2 avg grad: 0.7303526\n",
      "Layer 0 avg grad: -0.012690411\n",
      "Layer 1 avg grad: -0.17700016\n",
      "Layer 2 avg grad: 0.73039114\n",
      "Layer 0 avg grad: -0.014897702\n",
      "Layer 1 avg grad: -0.2825428\n",
      "Layer 2 avg grad: 0.7259692\n",
      "Layer 0 avg grad: -0.0103717465\n",
      "Layer 1 avg grad: -0.17774928\n",
      "Layer 2 avg grad: 0.7133192\n",
      "Layer 0 avg grad: -0.01079789\n",
      "Layer 1 avg grad: -0.22263922\n",
      "Layer 2 avg grad: 0.7306553\n",
      "Layer 0 avg grad: -0.01621132\n",
      "Layer 1 avg grad: -0.33941752\n",
      "Layer 2 avg grad: 0.78596\n",
      "Layer 0 avg grad: -0.008617515\n",
      "Layer 1 avg grad: -0.3090463\n",
      "Layer 2 avg grad: 0.7597839\n",
      "Layer 0 avg grad: -0.021434303\n",
      "Layer 1 avg grad: -0.30117637\n",
      "Layer 2 avg grad: 0.7681541\n",
      "Layer 0 avg grad: -0.018342936\n",
      "Layer 1 avg grad: -0.4140393\n",
      "Layer 2 avg grad: 0.79264355\n",
      "Layer 0 avg grad: -0.015058179\n",
      "Layer 1 avg grad: -0.32043633\n",
      "Layer 2 avg grad: 0.79821575\n",
      "Layer 0 avg grad: -0.013485884\n",
      "Layer 1 avg grad: -0.4203708\n",
      "Layer 2 avg grad: 0.7559043\n",
      "Layer 0 avg grad: -0.025170693\n",
      "Layer 1 avg grad: -0.38572243\n",
      "Layer 2 avg grad: 0.7852834\n",
      "Layer 0 avg grad: -0.016843788\n",
      "Layer 1 avg grad: -0.2843615\n",
      "Layer 2 avg grad: 0.8251385\n",
      "Layer 0 avg grad: -0.03073707\n",
      "Layer 1 avg grad: -0.36299562\n",
      "Layer 2 avg grad: 0.88610184\n",
      "Layer 0 avg grad: -0.021371193\n",
      "Layer 1 avg grad: -0.4838399\n",
      "Layer 2 avg grad: 0.79100484\n",
      "Layer 0 avg grad: -0.034884647\n",
      "Layer 1 avg grad: -0.47719476\n",
      "Layer 2 avg grad: 0.8813532\n",
      "Layer 0 avg grad: -0.032544814\n",
      "Layer 1 avg grad: -0.5839087\n",
      "Layer 2 avg grad: 0.8766839\n",
      "Layer 0 avg grad: -0.025754659\n",
      "Layer 1 avg grad: -0.36478874\n",
      "Layer 2 avg grad: 0.8771588\n",
      "Layer 0 avg grad: -0.037132517\n",
      "Layer 1 avg grad: -0.5773541\n",
      "Layer 2 avg grad: 0.8745948\n",
      "Layer 0 avg grad: -0.01746588\n",
      "Layer 1 avg grad: -0.60301995\n",
      "Layer 2 avg grad: 0.95679253\n",
      "Layer 0 avg grad: -0.045008704\n",
      "Layer 1 avg grad: -0.66592383\n",
      "Layer 2 avg grad: 0.94435483\n",
      "Layer 0 avg grad: -0.03711756\n",
      "Layer 1 avg grad: -0.64479744\n",
      "Layer 2 avg grad: 0.9327114\n",
      "Layer 0 avg grad: -0.036291704\n",
      "Layer 1 avg grad: -0.49528655\n",
      "Layer 2 avg grad: 0.9384764\n",
      "Layer 0 avg grad: -0.055277113\n",
      "Layer 1 avg grad: -0.71699953\n",
      "Layer 2 avg grad: 1.0459112\n",
      "Layer 0 avg grad: -0.046304196\n",
      "Layer 1 avg grad: -0.5986802\n",
      "Layer 2 avg grad: 1.0005656\n",
      "Layer 0 avg grad: -0.06596645\n",
      "Layer 1 avg grad: -0.7407542\n",
      "Layer 2 avg grad: 1.1005378\n",
      "Layer 0 avg grad: -0.05220388\n",
      "Layer 1 avg grad: -0.7053929\n",
      "Layer 2 avg grad: 1.1064619\n",
      "Layer 0 avg grad: -0.06985043\n",
      "Layer 1 avg grad: -0.81079376\n",
      "Layer 2 avg grad: 1.1403388\n",
      "Layer 0 avg grad: -0.06759759\n",
      "Layer 1 avg grad: -0.7384515\n",
      "Layer 2 avg grad: 1.1042846\n",
      "Layer 0 avg grad: -0.07392898\n",
      "Layer 1 avg grad: -0.89267164\n",
      "Layer 2 avg grad: 1.1848452\n",
      "Layer 0 avg grad: -0.083062485\n",
      "Layer 1 avg grad: -0.85402244\n",
      "Layer 2 avg grad: 1.1541532\n",
      "Layer 0 avg grad: -0.07249374\n",
      "Layer 1 avg grad: -0.90264213\n",
      "Layer 2 avg grad: 1.2357205\n",
      "Layer 0 avg grad: -0.08767577\n",
      "Layer 1 avg grad: -0.8873742\n",
      "Layer 2 avg grad: 1.2663463\n",
      "Layer 0 avg grad: -0.10117521\n",
      "Layer 1 avg grad: -0.95578367\n",
      "Layer 2 avg grad: 1.2759172\n",
      "Layer 0 avg grad: -0.0949011\n",
      "Layer 1 avg grad: -0.96011126\n",
      "Layer 2 avg grad: 1.3783501\n",
      "Layer 0 avg grad: -0.09257979\n",
      "Layer 1 avg grad: -0.91516984\n",
      "Layer 2 avg grad: 1.3461109\n",
      "Layer 0 avg grad: -0.08862515\n",
      "Layer 1 avg grad: -0.9464891\n",
      "Layer 2 avg grad: 1.3801154\n",
      "Layer 0 avg grad: -0.09455688\n",
      "Layer 1 avg grad: -0.9896486\n",
      "Layer 2 avg grad: 1.3255892\n",
      "Layer 0 avg grad: -0.111160606\n",
      "Layer 1 avg grad: -1.06096\n",
      "Layer 2 avg grad: 1.4160836\n",
      "Layer 0 avg grad: -0.09666739\n",
      "Layer 1 avg grad: -1.06672\n",
      "Layer 2 avg grad: 1.3578703\n",
      "Layer 0 avg grad: -0.113637306\n",
      "Layer 1 avg grad: -1.1560072\n",
      "Layer 2 avg grad: 1.3589584\n",
      "Layer 0 avg grad: -0.11391146\n",
      "Layer 1 avg grad: -0.9789076\n",
      "Layer 2 avg grad: 1.3779105\n",
      "Layer 0 avg grad: -0.111863084\n",
      "Layer 1 avg grad: -1.176988\n",
      "Layer 2 avg grad: 1.3965572\n",
      "Layer 0 avg grad: -0.09968532\n",
      "Layer 1 avg grad: -1.0124464\n",
      "Layer 2 avg grad: 1.3635218\n",
      "Layer 0 avg grad: -0.10195048\n",
      "Layer 1 avg grad: -0.9279993\n",
      "Layer 2 avg grad: 1.2597994\n",
      "Layer 0 avg grad: -0.10214683\n",
      "Layer 1 avg grad: -1.0439986\n",
      "Layer 2 avg grad: 1.2242813\n",
      "Layer 0 avg grad: -0.09095812\n",
      "Layer 1 avg grad: -0.9115072\n",
      "Layer 2 avg grad: 1.1924515\n",
      "Layer 0 avg grad: -0.10189347\n",
      "Layer 1 avg grad: -0.93600553\n",
      "Layer 2 avg grad: 1.1721914\n",
      "Layer 0 avg grad: -0.09612019\n",
      "Layer 1 avg grad: -0.85466915\n",
      "Layer 2 avg grad: 1.0469669\n",
      "Layer 0 avg grad: -0.07544228\n",
      "Layer 1 avg grad: -0.7267016\n",
      "Layer 2 avg grad: 1.027635\n",
      "Layer 0 avg grad: -0.0739787\n",
      "Layer 1 avg grad: -0.7072002\n",
      "Layer 2 avg grad: 0.96340865\n",
      "Layer 0 avg grad: -0.077289864\n",
      "Layer 1 avg grad: -0.7232415\n",
      "Layer 2 avg grad: 0.96172476\n",
      "Layer 0 avg grad: -0.07230409\n",
      "Layer 1 avg grad: -0.6726774\n",
      "Layer 2 avg grad: 0.78446704\n",
      "Layer 0 avg grad: -0.072187044\n",
      "Layer 1 avg grad: -0.68785566\n",
      "Layer 2 avg grad: 0.77611595\n",
      "Layer 0 avg grad: -0.0749066\n",
      "Layer 1 avg grad: -0.6974376\n",
      "Layer 2 avg grad: 0.90339005\n",
      "Layer 0 avg grad: -0.052522294\n",
      "Layer 1 avg grad: -0.49118558\n",
      "Layer 2 avg grad: 0.61391705\n",
      "Layer 0 avg grad: -0.06946375\n",
      "Layer 1 avg grad: -0.61588037\n",
      "Layer 2 avg grad: 0.7789773\n",
      "Layer 0 avg grad: -0.04648916\n",
      "Layer 1 avg grad: -0.47111022\n",
      "Layer 2 avg grad: 0.6372688\n",
      "Layer 0 avg grad: -0.04878973\n",
      "Layer 1 avg grad: -0.48397753\n",
      "Layer 2 avg grad: 0.61961174\n",
      "Layer 0 avg grad: -0.052538313\n",
      "Layer 1 avg grad: -0.41763806\n",
      "Layer 2 avg grad: 0.617649\n",
      "Layer 0 avg grad: -0.041848443\n",
      "Layer 1 avg grad: -0.37231484\n",
      "Layer 2 avg grad: 0.47372723\n",
      "Layer 0 avg grad: -0.0383374\n",
      "Layer 1 avg grad: -0.35519224\n",
      "Layer 2 avg grad: 0.38718504\n",
      "Layer 0 avg grad: -0.03626702\n",
      "Layer 1 avg grad: -0.29420266\n",
      "Layer 2 avg grad: 0.40133768\n",
      "Layer 0 avg grad: -0.035767816\n",
      "Layer 1 avg grad: -0.2885181\n",
      "Layer 2 avg grad: 0.42413983\n",
      "Layer 0 avg grad: -0.03007393\n",
      "Layer 1 avg grad: -0.2395675\n",
      "Layer 2 avg grad: 0.31790707\n",
      "Layer 0 avg grad: -0.02586544\n",
      "Layer 1 avg grad: -0.2170435\n",
      "Layer 2 avg grad: 0.30984557\n",
      "Layer 0 avg grad: -0.03510775\n",
      "Layer 1 avg grad: -0.31048518\n",
      "Layer 2 avg grad: 0.4038863\n",
      "Layer 0 avg grad: -0.0224992\n",
      "Layer 1 avg grad: -0.14935328\n",
      "Layer 2 avg grad: 0.18471752\n",
      "Layer 0 avg grad: -0.019957472\n",
      "Layer 1 avg grad: -0.21440364\n",
      "Layer 2 avg grad: 0.2754164\n",
      "Layer 0 avg grad: -0.010267342\n",
      "Layer 1 avg grad: -0.06306833\n",
      "Layer 2 avg grad: 0.11399976\n",
      "Layer 0 avg grad: -0.02334592\n",
      "Layer 1 avg grad: -0.1558648\n",
      "Layer 2 avg grad: 0.262865\n",
      "Layer 0 avg grad: -0.032125495\n",
      "Layer 1 avg grad: -0.27614072\n",
      "Layer 2 avg grad: 0.37051407\n",
      "Layer 0 avg grad: -0.030165857\n",
      "Layer 1 avg grad: -0.25166038\n",
      "Layer 2 avg grad: 0.31696177\n",
      "Layer 0 avg grad: -0.0155650275\n",
      "Layer 1 avg grad: -0.12578659\n",
      "Layer 2 avg grad: 0.15818773\n",
      "Layer 0 avg grad: -0.018124819\n",
      "Layer 1 avg grad: -0.14980952\n",
      "Layer 2 avg grad: 0.2037063\n",
      "Layer 0 avg grad: -0.015203921\n",
      "Layer 1 avg grad: -0.13244386\n",
      "Layer 2 avg grad: 0.20475367\n",
      "Layer 0 avg grad: -0.023242796\n",
      "Layer 1 avg grad: -0.16147079\n",
      "Layer 2 avg grad: 0.2691863\n",
      "Layer 0 avg grad: -0.007303964\n",
      "Layer 1 avg grad: -0.039566394\n",
      "Layer 2 avg grad: 0.08052464\n",
      "Layer 0 avg grad: -0.012035164\n",
      "Layer 1 avg grad: -0.105319485\n",
      "Layer 2 avg grad: 0.10451553\n",
      "Layer 0 avg grad: -0.011983387\n",
      "Layer 1 avg grad: -0.11472949\n",
      "Layer 2 avg grad: 0.1352124\n",
      "Layer 0 avg grad: -0.0013131297\n",
      "Layer 1 avg grad: -0.017300766\n",
      "Layer 2 avg grad: 0.09501384\n",
      "Layer 0 avg grad: -0.008383784\n",
      "Layer 1 avg grad: -0.046440583\n",
      "Layer 2 avg grad: 0.076767646\n",
      "Layer 0 avg grad: -0.007836297\n",
      "Layer 1 avg grad: -0.06799327\n",
      "Layer 2 avg grad: 0.12989648\n",
      "Layer 0 avg grad: -0.013058865\n",
      "Layer 1 avg grad: -0.09688161\n",
      "Layer 2 avg grad: 0.11807696\n",
      "Layer 0 avg grad: -0.020276798\n",
      "Layer 1 avg grad: -0.15196584\n",
      "Layer 2 avg grad: 0.16139086\n",
      "Layer 0 avg grad: -0.015266383\n",
      "Layer 1 avg grad: -0.108497925\n",
      "Layer 2 avg grad: 0.18645325\n",
      "Layer 0 avg grad: -0.0053341934\n",
      "Layer 1 avg grad: -0.029681185\n",
      "Layer 2 avg grad: 0.04592042\n",
      "Layer 0 avg grad: -0.016694093\n",
      "Layer 1 avg grad: -0.13854161\n",
      "Layer 2 avg grad: 0.19621627\n",
      "Layer 0 avg grad: -0.0073183817\n",
      "Layer 1 avg grad: -0.06933633\n",
      "Layer 2 avg grad: 0.08752188\n",
      "Layer 0 avg grad: -0.0020011852\n",
      "Layer 1 avg grad: -0.006827382\n",
      "Layer 2 avg grad: 0.020286554\n",
      "Layer 0 avg grad: -0.009640781\n",
      "Layer 1 avg grad: -0.09082937\n",
      "Layer 2 avg grad: 0.122479774\n",
      "Layer 0 avg grad: -0.0061152247\n",
      "Layer 1 avg grad: -0.0507796\n",
      "Layer 2 avg grad: 0.065523334\n",
      "Layer 0 avg grad: -0.006424984\n",
      "Layer 1 avg grad: -0.056945022\n",
      "Layer 2 avg grad: 0.07333469\n",
      "Layer 0 avg grad: 0.0004979944\n",
      "Layer 1 avg grad: 0.02088402\n",
      "Layer 2 avg grad: 0.004834429\n",
      "Layer 0 avg grad: -0.0061033624\n",
      "Layer 1 avg grad: -0.07177327\n",
      "Layer 2 avg grad: 0.070607476\n",
      "Layer 0 avg grad: -0.008920744\n",
      "Layer 1 avg grad: -0.083943956\n",
      "Layer 2 avg grad: 0.13027504\n",
      "Layer 0 avg grad: -0.0016096989\n",
      "Layer 1 avg grad: 0.005257727\n",
      "Layer 2 avg grad: -0.021041846\n",
      "Layer 0 avg grad: -0.001683435\n",
      "Layer 1 avg grad: -0.009326125\n",
      "Layer 2 avg grad: 0.014457509\n",
      "Layer 0 avg grad: -0.009338206\n",
      "Layer 1 avg grad: -0.085174374\n",
      "Layer 2 avg grad: 0.12165095\n",
      "Layer 0 avg grad: -0.0071769943\n",
      "Layer 1 avg grad: -0.025025502\n",
      "Layer 2 avg grad: 0.025149677\n",
      "Layer 0 avg grad: -0.006514998\n",
      "Layer 1 avg grad: -0.042620655\n",
      "Layer 2 avg grad: 0.0655443\n",
      "Layer 0 avg grad: -0.00844565\n",
      "Layer 1 avg grad: -0.048763808\n",
      "Layer 2 avg grad: 0.09490241\n",
      "Layer 0 avg grad: -0.011007842\n",
      "Layer 1 avg grad: -0.11148192\n",
      "Layer 2 avg grad: 0.11008004\n",
      "Layer 0 avg grad: -0.0053856415\n",
      "Layer 1 avg grad: -0.056193683\n",
      "Layer 2 avg grad: 0.045998123\n",
      "Layer 0 avg grad: 0.0007177376\n",
      "Layer 1 avg grad: 0.050629947\n",
      "Layer 2 avg grad: -0.016669612\n",
      "Layer 0 avg grad: -0.00881822\n",
      "Layer 1 avg grad: -0.038216844\n",
      "Layer 2 avg grad: 0.069511585\n",
      "Layer 0 avg grad: -0.0033370964\n",
      "Layer 1 avg grad: -0.029656751\n",
      "Layer 2 avg grad: 0.029302603\n",
      "Layer 0 avg grad: -0.011626932\n",
      "Layer 1 avg grad: -0.08970153\n",
      "Layer 2 avg grad: 0.11289193\n",
      "Layer 0 avg grad: -0.0057086493\n",
      "Layer 1 avg grad: -0.041675523\n",
      "Layer 2 avg grad: 0.050723385\n",
      "Layer 0 avg grad: -0.0053005894\n",
      "Layer 1 avg grad: -0.023350075\n",
      "Layer 2 avg grad: 0.06964388\n",
      "Layer 0 avg grad: -0.0044253967\n",
      "Layer 1 avg grad: -0.027957417\n",
      "Layer 2 avg grad: 0.033809226\n",
      "Layer 0 avg grad: -0.0014207155\n",
      "Layer 1 avg grad: -0.0021334605\n",
      "Layer 2 avg grad: 0.02642904\n",
      "Layer 0 avg grad: 0.00024490274\n",
      "Layer 1 avg grad: 0.0059457095\n",
      "Layer 2 avg grad: -0.0059732604\n",
      "Layer 0 avg grad: -0.005001188\n",
      "Layer 1 avg grad: -0.017347254\n",
      "Layer 2 avg grad: -0.006247974\n",
      "Layer 0 avg grad: -0.0062602605\n",
      "Layer 1 avg grad: -0.072584584\n",
      "Layer 2 avg grad: 0.09316315\n",
      "Layer 0 avg grad: -0.0035662833\n",
      "Layer 1 avg grad: -0.005568505\n",
      "Layer 2 avg grad: 0.018000375\n",
      "Layer 0 avg grad: -0.00761449\n",
      "Layer 1 avg grad: -0.041523132\n",
      "Layer 2 avg grad: 0.079333305\n",
      "Layer 0 avg grad: 0.0035354597\n",
      "Layer 1 avg grad: 0.010428775\n",
      "Layer 2 avg grad: -0.047146052\n",
      "Layer 0 avg grad: 0.0019840247\n",
      "Layer 1 avg grad: 0.028974231\n",
      "Layer 2 avg grad: -0.04867114\n",
      "Layer 0 avg grad: 0.00016181322\n",
      "Layer 1 avg grad: -0.0052097943\n",
      "Layer 2 avg grad: -0.005797708\n",
      "Layer 0 avg grad: -0.009393024\n",
      "Layer 1 avg grad: -0.06053663\n",
      "Layer 2 avg grad: 0.089268476\n",
      "Layer 0 avg grad: -0.011025192\n",
      "Layer 1 avg grad: -0.038411222\n",
      "Layer 2 avg grad: 0.062459137\n",
      "Layer 0 avg grad: -0.0029507827\n",
      "Layer 1 avg grad: -0.028406855\n",
      "Layer 2 avg grad: 0.042972762\n",
      "Layer 0 avg grad: -0.00434026\n",
      "Layer 1 avg grad: -0.034160506\n",
      "Layer 2 avg grad: 0.04272982\n",
      "Layer 0 avg grad: -0.0033661856\n",
      "Layer 1 avg grad: -0.015929077\n",
      "Layer 2 avg grad: 0.06523668\n",
      "Layer 0 avg grad: -0.0011994501\n",
      "Layer 1 avg grad: 0.024920927\n",
      "Layer 2 avg grad: -0.0047630053\n",
      "Layer 0 avg grad: 0.0018635278\n",
      "Layer 1 avg grad: 0.033662166\n",
      "Layer 2 avg grad: -0.048040994\n",
      "Layer 0 avg grad: -0.0026378408\n",
      "Layer 1 avg grad: -0.0036866912\n",
      "Layer 2 avg grad: 0.006976042\n",
      "Layer 0 avg grad: -0.001274116\n",
      "Layer 1 avg grad: -0.0042808275\n",
      "Layer 2 avg grad: -0.003913281\n",
      "Layer 0 avg grad: -0.0011097491\n",
      "Layer 1 avg grad: -0.022492962\n",
      "Layer 2 avg grad: 0.02260371\n",
      "Layer 0 avg grad: -0.0071048113\n",
      "Layer 1 avg grad: -0.068311855\n",
      "Layer 2 avg grad: 0.09730718\n",
      "Layer 0 avg grad: -0.009327524\n",
      "Layer 1 avg grad: -0.046817325\n",
      "Layer 2 avg grad: 0.08858856\n",
      "Layer 0 avg grad: -0.001958839\n",
      "Layer 1 avg grad: 0.0036888379\n",
      "Layer 2 avg grad: 0.037724327\n",
      "Layer 0 avg grad: -0.0044111614\n",
      "Layer 1 avg grad: -0.03398955\n",
      "Layer 2 avg grad: 0.04320678\n",
      "Layer 0 avg grad: 0.0021522008\n",
      "Layer 1 avg grad: 0.04305542\n",
      "Layer 2 avg grad: -0.038855717\n",
      "Layer 0 avg grad: -0.0022733086\n",
      "Layer 1 avg grad: -0.01611764\n",
      "Layer 2 avg grad: 0.020402916\n",
      "Layer 0 avg grad: -0.00080326624\n",
      "Layer 1 avg grad: 0.013292544\n",
      "Layer 2 avg grad: -0.017269216\n",
      "Layer 0 avg grad: 0.00052461127\n",
      "Layer 1 avg grad: 0.03735816\n",
      "Layer 2 avg grad: 0.0039698617\n",
      "Layer 0 avg grad: -0.0011374633\n",
      "Layer 1 avg grad: 0.018734738\n",
      "Layer 2 avg grad: -0.005202777\n",
      "Layer 0 avg grad: -0.0055777887\n",
      "Layer 1 avg grad: -0.0073035187\n",
      "Layer 2 avg grad: 0.02560538\n",
      "Layer 0 avg grad: -0.0034867737\n",
      "Layer 1 avg grad: -0.016914396\n",
      "Layer 2 avg grad: 0.0298695\n",
      "Layer 0 avg grad: 0.0046273917\n",
      "Layer 1 avg grad: 0.059355065\n",
      "Layer 2 avg grad: -0.043265644\n",
      "Layer 0 avg grad: -0.00096677773\n",
      "Layer 1 avg grad: 0.00051261147\n",
      "Layer 2 avg grad: 0.019339956\n",
      "Layer 0 avg grad: -0.001885076\n",
      "Layer 1 avg grad: 0.015718898\n",
      "Layer 2 avg grad: 0.015025727\n",
      "Layer 0 avg grad: -0.006148476\n",
      "Layer 1 avg grad: 0.01140537\n",
      "Layer 2 avg grad: 0.010665069\n",
      "Layer 0 avg grad: 0.001715852\n",
      "Layer 1 avg grad: 0.025792962\n",
      "Layer 2 avg grad: -0.030138193\n",
      "Layer 0 avg grad: 0.00094890606\n",
      "Layer 1 avg grad: 0.021906817\n",
      "Layer 2 avg grad: -0.03218241\n",
      "Layer 0 avg grad: 0.0018906192\n",
      "Layer 1 avg grad: 0.025344212\n",
      "Layer 2 avg grad: -0.0111486325\n",
      "Layer 0 avg grad: -0.0019065889\n",
      "Layer 1 avg grad: 0.0028968798\n",
      "Layer 2 avg grad: 0.017130353\n",
      "Layer 0 avg grad: -0.0042126216\n",
      "Layer 1 avg grad: -0.020170573\n",
      "Layer 2 avg grad: 0.0476481\n",
      "Layer 0 avg grad: -0.002248008\n",
      "Layer 1 avg grad: 0.0053881234\n",
      "Layer 2 avg grad: 0.010615747\n",
      "Layer 0 avg grad: -0.004417454\n",
      "Layer 1 avg grad: -0.024746472\n",
      "Layer 2 avg grad: 0.046561193\n",
      "Layer 0 avg grad: 0.006698313\n",
      "Layer 1 avg grad: 0.09467365\n",
      "Layer 2 avg grad: -0.09105924\n",
      "Layer 0 avg grad: 0.0059824134\n",
      "Layer 1 avg grad: 0.0729821\n",
      "Layer 2 avg grad: -0.06559112\n",
      "Layer 0 avg grad: -0.009626303\n",
      "Layer 1 avg grad: -0.07000486\n",
      "Layer 2 avg grad: 0.089969836\n",
      "Layer 0 avg grad: -0.0012025048\n",
      "Layer 1 avg grad: 0.013084032\n",
      "Layer 2 avg grad: -0.0020861987\n",
      "Layer 0 avg grad: -0.0070207454\n",
      "Layer 1 avg grad: -0.05014186\n",
      "Layer 2 avg grad: 0.06408467\n",
      "Layer 0 avg grad: -0.0036868784\n",
      "Layer 1 avg grad: -0.03743893\n",
      "Layer 2 avg grad: 0.04128715\n",
      "Layer 0 avg grad: -0.006256615\n",
      "Layer 1 avg grad: 0.0039076996\n",
      "Layer 2 avg grad: 0.014211135\n",
      "Layer 0 avg grad: 0.0062940586\n",
      "Layer 1 avg grad: 0.08230222\n",
      "Layer 2 avg grad: -0.053521972\n",
      "Layer 0 avg grad: -0.008931793\n",
      "Layer 1 avg grad: -0.03463852\n",
      "Layer 2 avg grad: 0.043084\n",
      "Layer 0 avg grad: -0.0045144227\n",
      "Layer 1 avg grad: -0.035611503\n",
      "Layer 2 avg grad: 0.03548503\n",
      "Layer 0 avg grad: 0.0053295936\n",
      "Layer 1 avg grad: 0.06435023\n",
      "Layer 2 avg grad: -0.06193903\n",
      "Layer 0 avg grad: -0.0024643487\n",
      "Layer 1 avg grad: 0.002272507\n",
      "Layer 2 avg grad: -0.004766493\n",
      "Layer 0 avg grad: 0.0034404849\n",
      "Layer 1 avg grad: 0.0346903\n",
      "Layer 2 avg grad: -0.019889697\n",
      "Layer 0 avg grad: 0.0063214055\n",
      "Layer 1 avg grad: 0.055094592\n",
      "Layer 2 avg grad: -0.058618188\n",
      "Layer 0 avg grad: -0.0027067799\n",
      "Layer 1 avg grad: -0.021644063\n",
      "Layer 2 avg grad: 0.02067781\n",
      "Layer 0 avg grad: 0.002713274\n",
      "Layer 1 avg grad: 0.054885462\n",
      "Layer 2 avg grad: -0.04001722\n",
      "Layer 0 avg grad: -0.0032817814\n",
      "Layer 1 avg grad: -0.020964134\n",
      "Layer 2 avg grad: 0.021561516\n",
      "Layer 0 avg grad: 0.003083311\n",
      "Layer 1 avg grad: 0.042087864\n",
      "Layer 2 avg grad: -0.032417007\n",
      "Layer 0 avg grad: -0.00024953173\n",
      "Layer 1 avg grad: 0.002741639\n",
      "Layer 2 avg grad: -0.004581452\n",
      "Layer 0 avg grad: -0.003598172\n",
      "Layer 1 avg grad: -0.03356682\n",
      "Layer 2 avg grad: 0.041826773\n",
      "Layer 0 avg grad: 0.0029845906\n",
      "Layer 1 avg grad: 0.03335905\n",
      "Layer 2 avg grad: -0.04171922\n",
      "Layer 0 avg grad: 0.008130894\n",
      "Layer 1 avg grad: 0.087068096\n",
      "Layer 2 avg grad: -0.085946344\n",
      "Layer 0 avg grad: 0.00032494016\n",
      "Layer 1 avg grad: 0.0017640401\n",
      "Layer 2 avg grad: 0.008337834\n",
      "Layer 0 avg grad: 0.0012988662\n",
      "Layer 1 avg grad: 0.030546851\n",
      "Layer 2 avg grad: -0.03375754\n",
      "Layer 0 avg grad: -0.0018763356\n",
      "Layer 1 avg grad: -0.010701679\n",
      "Layer 2 avg grad: -0.006251847\n",
      "Layer 0 avg grad: 0.0009866592\n",
      "Layer 1 avg grad: 0.035395086\n",
      "Layer 2 avg grad: -0.03595545\n",
      "Layer 0 avg grad: 0.0011183865\n",
      "Layer 1 avg grad: 0.03313659\n",
      "Layer 2 avg grad: -0.028025467\n",
      "Layer 0 avg grad: -0.00011298371\n",
      "Layer 1 avg grad: 0.012242163\n",
      "Layer 2 avg grad: 0.0011109086\n",
      "Layer 0 avg grad: 0.0039960607\n",
      "Layer 1 avg grad: 0.063593596\n",
      "Layer 2 avg grad: -0.050453275\n",
      "Layer 0 avg grad: -0.0010496038\n",
      "Layer 1 avg grad: 0.0035250934\n",
      "Layer 2 avg grad: -0.0031906876\n",
      "Layer 0 avg grad: -0.0035139318\n",
      "Layer 1 avg grad: -0.047110915\n",
      "Layer 2 avg grad: 0.040703267\n",
      "Layer 0 avg grad: -6.5967683e-06\n",
      "Layer 1 avg grad: 0.0019822067\n",
      "Layer 2 avg grad: -0.014746397\n",
      "Layer 0 avg grad: -0.0060228272\n",
      "Layer 1 avg grad: -0.028340066\n",
      "Layer 2 avg grad: 0.014956556\n",
      "Layer 0 avg grad: -0.007005525\n",
      "Layer 1 avg grad: -0.04141913\n",
      "Layer 2 avg grad: 0.0568451\n",
      "Layer 0 avg grad: 0.0015076852\n",
      "Layer 1 avg grad: 0.037944067\n",
      "Layer 2 avg grad: -0.021060823\n",
      "Layer 0 avg grad: 0.0025297522\n",
      "Layer 1 avg grad: 0.01851614\n",
      "Layer 2 avg grad: -0.010089029\n",
      "Layer 0 avg grad: -0.00012386368\n",
      "Layer 1 avg grad: 0.01709523\n",
      "Layer 2 avg grad: -0.002414093\n",
      "Layer 0 avg grad: -0.0061339643\n",
      "Layer 1 avg grad: -0.04471685\n",
      "Layer 2 avg grad: 0.035892412\n",
      "Layer 0 avg grad: 0.0005216573\n",
      "Layer 1 avg grad: 0.025063332\n",
      "Layer 2 avg grad: -0.02046862\n",
      "Layer 0 avg grad: -0.0065471195\n",
      "Layer 1 avg grad: -0.055034436\n",
      "Layer 2 avg grad: 0.058814712\n",
      "Layer 0 avg grad: 0.000506827\n",
      "Layer 1 avg grad: 0.0069162766\n",
      "Layer 2 avg grad: -0.02886231\n",
      "Layer 0 avg grad: 0.004088646\n",
      "Layer 1 avg grad: 0.048373077\n",
      "Layer 2 avg grad: -0.052085876\n",
      "Layer 0 avg grad: -0.00019049495\n",
      "Layer 1 avg grad: 0.018960081\n",
      "Layer 2 avg grad: -0.027924845\n",
      "Layer 0 avg grad: 0.006715996\n",
      "Layer 1 avg grad: 0.07196386\n",
      "Layer 2 avg grad: -0.08742318\n",
      "Layer 0 avg grad: 0.0040117833\n",
      "Layer 1 avg grad: 0.05110606\n",
      "Layer 2 avg grad: -0.050538678\n",
      "Layer 0 avg grad: 0.009895334\n",
      "Layer 1 avg grad: 0.091405146\n",
      "Layer 2 avg grad: -0.12368957\n",
      "Layer 0 avg grad: -0.0030655756\n",
      "Layer 1 avg grad: -0.03288914\n",
      "Layer 2 avg grad: 0.031249661\n",
      "Layer 0 avg grad: -0.01002766\n",
      "Layer 1 avg grad: -0.08475438\n",
      "Layer 2 avg grad: 0.08339584\n",
      "Layer 0 avg grad: -0.005004465\n",
      "Layer 1 avg grad: -0.0025622202\n",
      "Layer 2 avg grad: 0.025599724\n",
      "Layer 0 avg grad: 0.00090372155\n",
      "Layer 1 avg grad: -0.0080637615\n",
      "Layer 2 avg grad: 0.0041258456\n",
      "Layer 0 avg grad: 5.922653e-05\n",
      "Layer 1 avg grad: 0.0330199\n",
      "Layer 2 avg grad: -0.007314779\n",
      "Layer 0 avg grad: 0.0032653026\n",
      "Layer 1 avg grad: 0.047809005\n",
      "Layer 2 avg grad: -0.030562783\n",
      "Layer 0 avg grad: 0.0031462605\n",
      "Layer 1 avg grad: 0.037675552\n",
      "Layer 2 avg grad: -0.044117197\n",
      "Layer 0 avg grad: -0.010486539\n",
      "Layer 1 avg grad: -0.072519355\n",
      "Layer 2 avg grad: 0.094094925\n",
      "Layer 0 avg grad: -0.0072841155\n",
      "Layer 1 avg grad: -0.049590487\n",
      "Layer 2 avg grad: 0.08820267\n",
      "Layer 0 avg grad: -0.006787756\n",
      "Layer 1 avg grad: -0.046999943\n",
      "Layer 2 avg grad: 0.06306845\n",
      "Layer 0 avg grad: -0.002411283\n",
      "Layer 1 avg grad: 0.012285038\n",
      "Layer 2 avg grad: -0.005824395\n",
      "Layer 0 avg grad: -0.002912128\n",
      "Layer 1 avg grad: -0.0050051734\n",
      "Layer 2 avg grad: 0.013551342\n",
      "Layer 0 avg grad: 0.0028022593\n",
      "Layer 1 avg grad: 0.033492412\n",
      "Layer 2 avg grad: -0.027207566\n",
      "Layer 0 avg grad: -0.0011129952\n",
      "Layer 1 avg grad: 0.021572893\n",
      "Layer 2 avg grad: -0.004179127\n",
      "Layer 0 avg grad: -0.0019494897\n",
      "Layer 1 avg grad: 0.0048932\n",
      "Layer 2 avg grad: 0.0025323217\n",
      "Layer 0 avg grad: 0.005741003\n",
      "Layer 1 avg grad: 0.05759926\n",
      "Layer 2 avg grad: -0.06436881\n",
      "Layer 0 avg grad: 0.00024712586\n",
      "Layer 1 avg grad: 0.007019816\n",
      "Layer 2 avg grad: -0.015932236\n",
      "Layer 0 avg grad: -0.01528378\n",
      "Layer 1 avg grad: -0.118653074\n",
      "Layer 2 avg grad: 0.119881235\n",
      "Layer 0 avg grad: -0.00095562264\n",
      "Layer 1 avg grad: -5.6486024e-05\n",
      "Layer 2 avg grad: -0.0042453445\n",
      "Layer 0 avg grad: -0.002721089\n",
      "Layer 1 avg grad: 0.002373707\n",
      "Layer 2 avg grad: 0.0054522124\n",
      "Layer 0 avg grad: 0.00046308865\n",
      "Layer 1 avg grad: 0.015302071\n",
      "Layer 2 avg grad: -0.01720055\n",
      "Layer 0 avg grad: 0.0024831002\n",
      "Layer 1 avg grad: 0.013760577\n",
      "Layer 2 avg grad: -0.005800334\n",
      "Layer 0 avg grad: -0.0018702867\n",
      "Layer 1 avg grad: -0.004198103\n",
      "Layer 2 avg grad: 0.0004685743\n",
      "Layer 0 avg grad: -9.5552576e-05\n",
      "Layer 1 avg grad: 0.022078197\n",
      "Layer 2 avg grad: -0.01585418\n",
      "Layer 0 avg grad: 0.0021910279\n",
      "Layer 1 avg grad: 0.010087127\n",
      "Layer 2 avg grad: -0.040409885\n",
      "Layer 0 avg grad: -0.0031558624\n",
      "Layer 1 avg grad: -0.031678982\n",
      "Layer 2 avg grad: 0.01627588\n",
      "Layer 0 avg grad: -0.002381959\n",
      "Layer 1 avg grad: -0.014986901\n",
      "Layer 2 avg grad: 0.025252778\n",
      "Layer 0 avg grad: -0.007406212\n",
      "Layer 1 avg grad: -0.05632826\n",
      "Layer 2 avg grad: 0.08906939\n",
      "Layer 0 avg grad: -0.0035774782\n",
      "Layer 1 avg grad: -0.0097546335\n",
      "Layer 2 avg grad: 0.0018669311\n",
      "Layer 0 avg grad: -0.0064049372\n",
      "Layer 1 avg grad: -0.022428226\n",
      "Layer 2 avg grad: 0.07555697\n",
      "Layer 0 avg grad: 0.0041113314\n",
      "Layer 1 avg grad: 0.058513675\n",
      "Layer 2 avg grad: -0.046260186\n",
      "Layer 0 avg grad: -0.0021642528\n",
      "Layer 1 avg grad: -0.003842796\n",
      "Layer 2 avg grad: 0.01578816\n",
      "Layer 0 avg grad: -0.0029317262\n",
      "Layer 1 avg grad: -0.07304791\n",
      "Layer 2 avg grad: 0.036713477\n",
      "Layer 0 avg grad: -0.004840158\n",
      "Layer 1 avg grad: -0.01730277\n",
      "Layer 2 avg grad: 0.02465684\n",
      "Layer 0 avg grad: -0.0013924347\n",
      "Layer 1 avg grad: -0.030525174\n",
      "Layer 2 avg grad: 0.024186907\n",
      "Layer 0 avg grad: -0.0036666442\n",
      "Layer 1 avg grad: -0.0056361263\n",
      "Layer 2 avg grad: 0.02714628\n",
      "Layer 0 avg grad: -0.006773923\n",
      "Layer 1 avg grad: -0.025088437\n",
      "Layer 2 avg grad: 0.040030934\n",
      "Layer 0 avg grad: 0.002774381\n",
      "Layer 1 avg grad: 0.055643138\n",
      "Layer 2 avg grad: -0.06341734\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mJaxStackTraceBeforeTransformation\u001b[39m         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen runpy>:198\u001b[39m, in \u001b[36m_run_module_as_main\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen runpy>:88\u001b[39m, in \u001b[36m_run_code\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel_launcher.py:18\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mipykernel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m kernelapp \u001b[38;5;28;01mas\u001b[39;00m app\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m app.launch_new_instance()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\traitlets\\config\\application.py:1075\u001b[39m, in \u001b[36mlaunch_instance\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1074\u001b[39m app.initialize(argv)\n\u001b[32m-> \u001b[39m\u001b[32m1075\u001b[39m app.start()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelapp.py:739\u001b[39m, in \u001b[36mstart\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    738\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m739\u001b[39m     \u001b[38;5;28mself\u001b[39m.io_loop.start()\n\u001b[32m    740\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tornado\\platform\\asyncio.py:205\u001b[39m, in \u001b[36mstart\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mstart\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m     \u001b[38;5;28mself\u001b[39m.asyncio_loop.run_forever()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jseto\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py:608\u001b[39m, in \u001b[36mrun_forever\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    607\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m608\u001b[39m     \u001b[38;5;28mself\u001b[39m._run_once()\n\u001b[32m    609\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stopping:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jseto\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py:1936\u001b[39m, in \u001b[36m_run_once\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1935\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1936\u001b[39m         handle._run()\n\u001b[32m   1937\u001b[39m handle = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jseto\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\events.py:84\u001b[39m, in \u001b[36m_run\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     \u001b[38;5;28mself\u001b[39m._context.run(\u001b[38;5;28mself\u001b[39m._callback, *\u001b[38;5;28mself\u001b[39m._args)\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mSystemExit\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelbase.py:545\u001b[39m, in \u001b[36mdispatch_queue\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    544\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m545\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.process_one()\n\u001b[32m    546\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelbase.py:534\u001b[39m, in \u001b[36mprocess_one\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    533\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m dispatch(*args)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelbase.py:437\u001b[39m, in \u001b[36mdispatch_shell\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    436\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m inspect.isawaitable(result):\n\u001b[32m--> \u001b[39m\u001b[32m437\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m result\n\u001b[32m    438\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\ipkernel.py:362\u001b[39m, in \u001b[36mexecute_request\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    361\u001b[39m \u001b[38;5;28mself\u001b[39m._associate_new_top_level_threads_with(parent_header)\n\u001b[32m--> \u001b[39m\u001b[32m362\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().execute_request(stream, ident, parent)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelbase.py:778\u001b[39m, in \u001b[36mexecute_request\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    777\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inspect.isawaitable(reply_content):\n\u001b[32m--> \u001b[39m\u001b[32m778\u001b[39m     reply_content = \u001b[38;5;28;01mawait\u001b[39;00m reply_content\n\u001b[32m    780\u001b[39m \u001b[38;5;66;03m# Flush output before sending the reply.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\ipkernel.py:449\u001b[39m, in \u001b[36mdo_execute\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m accepts_params[\u001b[33m\"\u001b[39m\u001b[33mcell_id\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     res = shell.run_cell(\n\u001b[32m    450\u001b[39m         code,\n\u001b[32m    451\u001b[39m         store_history=store_history,\n\u001b[32m    452\u001b[39m         silent=silent,\n\u001b[32m    453\u001b[39m         cell_id=cell_id,\n\u001b[32m    454\u001b[39m     )\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\zmqshell.py:549\u001b[39m, in \u001b[36mrun_cell\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    548\u001b[39m \u001b[38;5;28mself\u001b[39m._last_traceback = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().run_cell(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jseto\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3098\u001b[39m, in \u001b[36mrun_cell\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   3097\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3098\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._run_cell(\n\u001b[32m   3099\u001b[39m         raw_cell, store_history, silent, shell_futures, cell_id\n\u001b[32m   3100\u001b[39m     )\n\u001b[32m   3101\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jseto\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3153\u001b[39m, in \u001b[36m_run_cell\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   3152\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3153\u001b[39m     result = runner(coro)\n\u001b[32m   3154\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jseto\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\async_helpers.py:128\u001b[39m, in \u001b[36m_pseudo_sync_runner\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     coro.send(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jseto\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3365\u001b[39m, in \u001b[36mrun_cell_async\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   3362\u001b[39m interactivity = \u001b[33m\"\u001b[39m\u001b[33mnone\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m silent \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ast_node_interactivity\n\u001b[32m-> \u001b[39m\u001b[32m3365\u001b[39m has_raised = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.run_ast_nodes(code_ast.body, cell_name,\n\u001b[32m   3366\u001b[39m        interactivity=interactivity, compiler=compiler, result=result)\n\u001b[32m   3368\u001b[39m \u001b[38;5;28mself\u001b[39m.last_execution_succeeded = \u001b[38;5;129;01mnot\u001b[39;00m has_raised\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jseto\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3610\u001b[39m, in \u001b[36mrun_ast_nodes\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   3609\u001b[39m     asy = compare(code)\n\u001b[32m-> \u001b[39m\u001b[32m3610\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.run_code(code, result, async_=asy):\n\u001b[32m   3611\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jseto\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3670\u001b[39m, in \u001b[36mrun_code\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   3669\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3670\u001b[39m         exec(code_obj, \u001b[38;5;28mself\u001b[39m.user_global_ns, \u001b[38;5;28mself\u001b[39m.user_ns)\n\u001b[32m   3671\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   3672\u001b[39m     \u001b[38;5;66;03m# Reset our crash handler in place\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m10\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     datas.append(my_model.train_epoch(b_x, b_y, lr=\u001b[32m1e-3\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 69\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch_num \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(y)):\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m     mse, grads = jax.value_and_grad(\u001b[38;5;28mself\u001b[39m.loss_static, argnums=(\u001b[32m0\u001b[39m))((\u001b[38;5;28mself\u001b[39m.layers, \u001b[38;5;28mself\u001b[39m.biases), x[batch_num], y[batch_num])\n\u001b[32m     71\u001b[39m     losses.append(mse)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 49\u001b[39m, in \u001b[36mloss_static\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(b) - \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     x = relu(x)\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mrelu\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mrelu\u001b[39m(x):\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m jnp.where(x <= \u001b[32m0\u001b[39m, \u001b[32m1e-2\u001b[39m * x, x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jseto\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\jax\\_src\\numpy\\array_methods.py:1060\u001b[39m, in \u001b[36mop\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1059\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mop\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args):\n\u001b[32m-> \u001b[39m\u001b[32m1060\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.aval, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)(\u001b[38;5;28mself\u001b[39m, *args)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jseto\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\jax\\_src\\numpy\\array_methods.py:579\u001b[39m, in \u001b[36mdeferring_binary_op\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    578\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, _accepted_binop_types):\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m binary_op(*args)\n\u001b[32m    580\u001b[39m \u001b[38;5;66;03m# Note: don't use isinstance here, because we don't want to raise for\u001b[39;00m\n\u001b[32m    581\u001b[39m \u001b[38;5;66;03m# subclasses, e.g. NamedTuple objects that may override operators.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jseto\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\jax\\_src\\numpy\\ufunc_api.py:180\u001b[39m, in \u001b[36m__call__\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    179\u001b[39m call = \u001b[38;5;28mself\u001b[39m.__static_props[\u001b[33m'\u001b[39m\u001b[33mcall\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_vectorized\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m call(*args)\n",
      "\u001b[31mJaxStackTraceBeforeTransformation\u001b[39m: KeyboardInterrupt\n\nThe preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.\n\n--------------------",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m datas = []\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m10\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     datas.append(\u001b[43mmy_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-3\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 69\u001b[39m, in \u001b[36mModel.train_epoch\u001b[39m\u001b[34m(self, x, y, lr)\u001b[39m\n\u001b[32m     66\u001b[39m x = np.array(x)\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch_num \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(y)):\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m     mse, grads = \u001b[43mjax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalue_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloss_static\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margnums\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbiases\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch_num\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch_num\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m     losses.append(mse)\n\u001b[32m     73\u001b[39m    \u001b[38;5;66;03m#0 contains weights and 1 contains the bias grads. \u001b[39;00m\n\u001b[32m     74\u001b[39m     \u001b[38;5;66;03m# print([i.shape for i in grads[1]])\u001b[39;00m\n\u001b[32m     75\u001b[39m     \u001b[38;5;66;03m# print([i.shape for i in grads[0]])\u001b[39;00m\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jseto\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\jax\\_src\\api.py:514\u001b[39m, in \u001b[36mvalue_and_grad.<locals>.value_and_grad_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    512\u001b[39m _check_scalar(ans)\n\u001b[32m    513\u001b[39m tree_map(partial(_check_output_dtype_grad, holomorphic), ans)\n\u001b[32m--> \u001b[39m\u001b[32m514\u001b[39m g = \u001b[43mvjp_py\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlax_internal\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mans\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    515\u001b[39m g = g[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(argnums, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m g\n\u001b[32m    516\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_aux:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jseto\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\jax\\_src\\tree_util.py:471\u001b[39m, in \u001b[36m_HashableCallableShim.__call__\u001b[39m\u001b[34m(self, *args, **kw)\u001b[39m\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kw):\n\u001b[32m--> \u001b[39m\u001b[32m471\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jseto\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\jax\\_src\\api.py:1988\u001b[39m, in \u001b[36m_vjp_pullback_wrapper\u001b[39m\u001b[34m(name, out_primal_avals, io_tree, fun, *py_args_)\u001b[39m\n\u001b[32m   1981\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m core.typecompat(ct_aval, ct_aval_expected) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m   1982\u001b[39m       \u001b[38;5;129;01mnot\u001b[39;00m _temporary_dtype_exception(ct_aval, ct_aval_expected)):\n\u001b[32m   1983\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1984\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33munexpected JAX type (e.g. shape/dtype) for argument to vjp function: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1985\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mgot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mct_aval.str_short()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, but expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mct_aval_expected.str_short()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1986\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mbecause the corresponding output of the function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m had JAX type \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1987\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maval.str_short()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1988\u001b[39m ans = \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1989\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tree_unflatten(out_tree, ans)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jseto\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\jax\\_src\\tree_util.py:471\u001b[39m, in \u001b[36m_HashableCallableShim.__call__\u001b[39m\u001b[34m(self, *args, **kw)\u001b[39m\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kw):\n\u001b[32m--> \u001b[39m\u001b[32m471\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jseto\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\jax\\_src\\interpreters\\ad.py:291\u001b[39m, in \u001b[36mvjp.<locals>.unbound_vjp\u001b[39m\u001b[34m(pvals, jaxpr, consts, *cts)\u001b[39m\n\u001b[32m    289\u001b[39m cts = \u001b[38;5;28mtuple\u001b[39m(ct \u001b[38;5;28;01mfor\u001b[39;00m ct, pval \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(cts, pvals) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pval.is_known())\n\u001b[32m    290\u001b[39m dummy_args = [UndefinedPrimal(v.aval) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m jaxpr.invars]\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m arg_cts = \u001b[43mbackward_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjaxpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdummy_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(instantiate_zeros, arg_cts)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jseto\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\jax\\_src\\interpreters\\ad.py:413\u001b[39m, in \u001b[36mbackward_pass\u001b[39m\u001b[34m(jaxpr, transform_stack, consts, primals_in, cotangents_in)\u001b[39m\n\u001b[32m    411\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    412\u001b[39m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m413\u001b[39m     cts_out = \u001b[43mget_primitive_transpose\u001b[49m\u001b[43m(\u001b[49m\u001b[43meqn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprimitive\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcts_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43minvals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43meqn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    415\u001b[39m   \u001b[38;5;28;01mexcept\u001b[39;00m core.ShardingTypeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    416\u001b[39m     extra_msg = (\u001b[33m\"\u001b[39m\u001b[33mThis is a potential JAX bug. Please file an issue at\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    417\u001b[39m                  \u001b[33m\"\u001b[39m\u001b[33m https://github.com/jax-ml/jax/issues\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jseto\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\jax\\_src\\pjit.py:2533\u001b[39m, in \u001b[36m_pjit_transpose\u001b[39m\u001b[34m(cts_in, jaxpr, in_shardings, out_shardings, in_layouts, out_layouts, donated_invars, ctx_mesh, name, keep_unused, inline, compiler_options_kvs, *primals_in)\u001b[39m\n\u001b[32m   2530\u001b[39m   transpose_out_layouts = (\u001b[38;5;28;01mNone\u001b[39;00m,) * num_attr_outs + transpose_out_layouts\n\u001b[32m   2532\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2533\u001b[39m   nz_cts_out = \u001b[43mpjit_p\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2534\u001b[39m \u001b[43m      \u001b[49m\u001b[43m*\u001b[49m\u001b[43mprimals_and_nz_cts_in\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2535\u001b[39m \u001b[43m      \u001b[49m\u001b[43mjaxpr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtranspose_jaxpr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2536\u001b[39m \u001b[43m      \u001b[49m\u001b[43min_shardings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtranspose_in_shardings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2537\u001b[39m \u001b[43m      \u001b[49m\u001b[43mout_shardings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtranspose_out_shardings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2538\u001b[39m \u001b[43m      \u001b[49m\u001b[43min_layouts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtranspose_in_layouts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2539\u001b[39m \u001b[43m      \u001b[49m\u001b[43mout_layouts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtranspose_out_layouts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2540\u001b[39m \u001b[43m      \u001b[49m\u001b[43mdonated_invars\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprimals_and_nz_cts_in\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2541\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mctx_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2542\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2543\u001b[39m \u001b[43m      \u001b[49m\u001b[43mkeep_unused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2544\u001b[39m \u001b[43m      \u001b[49m\u001b[43minline\u001b[49m\u001b[43m=\u001b[49m\u001b[43minline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2545\u001b[39m \u001b[43m      \u001b[49m\u001b[43mcompiler_options_kvs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompiler_options_kvs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2546\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m dispatch.InternalFloatingPointError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2547\u001b[39m   \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mInvalid nan value encountered in the backward pass of a jax.jit \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2548\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mfunction. Calling the de-optimized backward pass.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jseto\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\jax\\_src\\core.py:496\u001b[39m, in \u001b[36mPrimitive.bind\u001b[39m\u001b[34m(self, *args, **params)\u001b[39m\n\u001b[32m    494\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **params):\n\u001b[32m    495\u001b[39m   args = args \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.skip_canonicalization \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(canonicalize_value, args)\n\u001b[32m--> \u001b[39m\u001b[32m496\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_true_bind\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jseto\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\jax\\_src\\core.py:512\u001b[39m, in \u001b[36mPrimitive._true_bind\u001b[39m\u001b[34m(self, *args, **params)\u001b[39m\n\u001b[32m    510\u001b[39m trace_ctx.set_trace(eval_trace)\n\u001b[32m    511\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m512\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbind_with_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprev_trace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    513\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    514\u001b[39m   trace_ctx.set_trace(prev_trace)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jseto\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\jax\\_src\\core.py:517\u001b[39m, in \u001b[36mPrimitive.bind_with_trace\u001b[39m\u001b[34m(self, trace, args, params)\u001b[39m\n\u001b[32m    516\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mbind_with_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m, trace, args, params):\n\u001b[32m--> \u001b[39m\u001b[32m517\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrace\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess_primitive\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jseto\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\jax\\_src\\core.py:1017\u001b[39m, in \u001b[36mEvalTrace.process_primitive\u001b[39m\u001b[34m(self, primitive, args, params)\u001b[39m\n\u001b[32m   1015\u001b[39m args = \u001b[38;5;28mmap\u001b[39m(full_lower, args)\n\u001b[32m   1016\u001b[39m check_eval_args(args)\n\u001b[32m-> \u001b[39m\u001b[32m1017\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprimitive\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimpl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jseto\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\jax\\_src\\pjit.py:1875\u001b[39m, in \u001b[36m_pjit_call_impl\u001b[39m\u001b[34m(jaxpr, in_shardings, out_shardings, in_layouts, out_layouts, donated_invars, ctx_mesh, name, keep_unused, inline, compiler_options_kvs, *args)\u001b[39m\n\u001b[32m   1867\u001b[39m donated_argnums = \u001b[38;5;28mtuple\u001b[39m(i \u001b[38;5;28;01mfor\u001b[39;00m i, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(donated_invars) \u001b[38;5;28;01mif\u001b[39;00m d)\n\u001b[32m   1868\u001b[39m cache_key = pxla.JitGlobalCppCacheKeys(\n\u001b[32m   1869\u001b[39m     donate_argnums=donated_argnums, donate_argnames=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1870\u001b[39m     device=\u001b[38;5;28;01mNone\u001b[39;00m, backend=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1873\u001b[39m     in_layouts_treedef=\u001b[38;5;28;01mNone\u001b[39;00m, in_layouts_leaves=in_layouts,\n\u001b[32m   1874\u001b[39m     out_layouts_treedef=\u001b[38;5;28;01mNone\u001b[39;00m, out_layouts_leaves=out_layouts)\n\u001b[32m-> \u001b[39m\u001b[32m1875\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mxc\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_xla\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpjit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1876\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall_impl_cache_miss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1877\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtree_util\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdispatch_registry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpxla\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcc_shard_arg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1878\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_get_cpp_global_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcache_key\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontains_explicit_attributes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "my_model = Model(28*28, 10, [4, 8])\n",
    "datas = []\n",
    "for _epoch in range(10):\n",
    "    datas.append(my_model.train_epoch(b_x, b_y, lr=1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b0d5a9-29d0-4a33-8ff4-145ca4db75a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline  \n",
    "\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "pic = 1\n",
    "for i, img in enumerate(x_test[:10]):\n",
    "  plt.subplot(2, 5, pic)\n",
    "  plt.axis('off')\n",
    "  predicted = my_model.fd(img.flat)\n",
    "  keras_pred = model.predict(img.reshape(1, 28*28))\n",
    "  plt.title(f\"T {y_test[i]} mine {np.argmax(predicted)} keras {np.argmax(keras_pred)} \")\n",
    "  plt.imshow(img)\n",
    "  pic+= 1\n",
    "plt.show()\n",
    "#60% acc. Considering this is from nearly scratch not terrible "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04591267-72f2-427e-bd16-da0eca6dd078",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
