{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24a9cc12-2578-4aef-8ee6-884ffab4f3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from grad import *\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc4ff80a-6930-4829-aa43-4236da9d571d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_loss(truth, model):\n",
    "    ''' Simple L2 loss for a single sample.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        truth : Sequence[float]\n",
    "            A single data point.\n",
    "              \n",
    "        model : Sequence[float]\n",
    "            The parameters of a model.\n",
    "              \n",
    "        Returns\n",
    "        -------\n",
    "        Number : the L2 loss (squared error) of `model_params` evaluated on `truth`\n",
    "    '''\n",
    "    l = truth[0] - model[0] - sum(truth[i]*model[i] for i in range(1, len(model)))\n",
    "    return l**2 if l.data > 0 else (-1*l)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a1517de-1c78-4229-ace8-9ca97c5d19ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, data_set, loss_fn, lr=0.001):\n",
    "    ''' Train a model for a single pass (epoch) through the provided data.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        model : Sequence[Number]\n",
    "            The parameters of a model\n",
    "            \n",
    "        data_set : Sequence[Sequence[float]]\n",
    "            The datapoints in a dataset\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        float : the mean loss for the epoch\n",
    "    '''\n",
    "    # compute the mean error over the dataset\n",
    "    mean_loss = sum(loss_fn(sample, model) for sample in data_set) / len(data_set)\n",
    "    \n",
    "    # compute gradients for our parameters\n",
    "    mean_loss.null_gradients()\n",
    "    mean_loss.backprop()\n",
    "    \n",
    "    # update the model parameters using gradient descent\n",
    "    for param in model:\n",
    "        # recall: param.grad is d(L)/d(param)\n",
    "        # thus this computes:\n",
    "        # param_new = param_old - step-size * d(L)/d(param) \n",
    "        param.data -= lr*param.grad  \n",
    "        \n",
    "    # return the loss for visualization\n",
    "    return mean_loss.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f21584e9-684c-41a7-b705-dd07612aae02",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "def weight_matrix(shape, stupid=False):\n",
    "    \"\"\"weight matrix thingy.give dims. Not 0.\"\"\"\n",
    "    # np.as_array(Number(np.random.rand()) for i in range(M*N))\n",
    "    number = 1\n",
    "    if(type(shape) == int):\n",
    "        shape = [shape]\n",
    "    for i in shape:\n",
    "        number*= i\n",
    "    if stupid:\n",
    "        return np.array([Number(i / 10) for i in range(number)]).reshape(*shape)\n",
    "    return np.array([Number(np.random.uniform(low=-.5, high=.5, size=None)) for i in range(number)]).reshape(*shape)\n",
    " \n",
    "def get_grads(x):\n",
    "    vectorized_grad = np.vectorize(lambda x : x.grad) #not true vecotrization just for readibility\n",
    "    return vectorized_grad(x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return np.vectorize(lambda x: 1/(1+math.e**-x))(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1818126-0bf2-4043-afd4-49f3025297f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    '''this only works w/ MSE and sigmoid because dik how to topological sort'''\n",
    "    \n",
    "    def __init__(self, input_size, output_size, hidden_layers, stupid=False):\n",
    "        '''\n",
    "        Takes list of # of things in their layers.\n",
    "        Layers are outputs?\n",
    "        '''\n",
    "        self.layer_sizes = hidden_layers\n",
    "        self.layers = []\n",
    "        self.biases = []\n",
    "\n",
    "        #Hidden states is after the *weight but before sigmoid\n",
    "        self.hidden_states = []\n",
    "\n",
    "        self.hidden_states_sigmoid = []\n",
    "        \n",
    "        prev = input_size\n",
    "        for hidden_layer in hidden_layers:\n",
    "            # self.layers, weight_matrix([prev, hidden_layer])\n",
    "            self.layers.append(weight_matrix([prev, hidden_layer], stupid))\n",
    "\n",
    "            self.biases.append(weight_matrix(hidden_layer, stupid))\n",
    "            prev  = hidden_layer\n",
    "        self.biases.append(weight_matrix([output_size]))\n",
    "        self.layers.append(weight_matrix([prev, output_size]))\n",
    " \n",
    "\n",
    "        # print(\"layers \"  + str(self.layers))\n",
    "        # print(\"biases \" + str(self.biases))\n",
    "\n",
    "    def fd(self, x):\n",
    "        '''f pass with input. Input has to be flat like a pancake'''\n",
    "        #sigma sigma boy.\n",
    "        self.hidden_states_sigmoid = []\n",
    "        self.hidden_states = []\n",
    "        self.input = x\n",
    "        for i in range(len(self.layers)):\n",
    "            # print(np.max(x))\n",
    "            x = x @ self.layers[i]\n",
    "            x += self.biases[i]\n",
    "            self.hidden_states.append(x)\n",
    "            x = sigmoid(x)\n",
    "            self.hidden_states_sigmoid.append(x)\n",
    "\n",
    "        return x\n",
    "        \n",
    "    def train_epoch(self, x, y, lr=10**-2):\n",
    "        '''\n",
    "        f pass and then uh gradient descent?\n",
    "\n",
    "        x: Input. Again, flat as a pancake.\n",
    "        y: the goal. In sparese tensor. \n",
    "        lr: how quick it learns\n",
    "        '''\n",
    "        num_correct = 0\n",
    "        losses = []\n",
    "        for i in range(len(y)):\n",
    "            # print(x)\n",
    "            pred = self.fd(x[i])\n",
    "            mse = np.sum((pred * pred -  y[i] *  y[i] + 2 * np.dot(pred, y[i]))) / len(y)\n",
    "            self.backprop(mse, y[i], lr)\n",
    "\n",
    "            num_correct += np.argmax(pred) == np.argmax(y[i])\n",
    "            losses.append(mse)\n",
    "\n",
    "            print(f\"Acc: {num_correct/len(y)} Avg loss: {sum(losses)/len(y)}\")\n",
    "\n",
    "\n",
    "        # print(\"finished backpropring gradiests\")\n",
    "\n",
    "    def backprop(self, loss, y, lr):\n",
    "        null_gradients(self.biases)\n",
    "        null_gradients(self.layers)\n",
    "\n",
    "         \n",
    "        loss.null_gradients(recursive=False)\n",
    " \n",
    "        #And here comes the hard part.\n",
    "        for i in range(len(self.hidden_states_sigmoid[-1])):\n",
    "            #the last one gets special treatment due to the mse. help this is so scuffed :sob:\n",
    "            self.hidden_states_sigmoid[-1][i].grad = 2 * (self.hidden_states_sigmoid[-1][i]  - y[i])\n",
    "\n",
    "        \n",
    "        for k in range(len(self.hidden_states_sigmoid) - 1,0 , -1):\n",
    "            curr_h = self.hidden_states[k]\n",
    "            curr_hs = self.hidden_states_sigmoid[k]\n",
    "            #Get the gradients of the hidden layer : h_k[i].grad = sh_k[i].sigmoid(h_k[i]) * ( 1 - sigmoid(h_k[i]))\n",
    "            for i in range(len(curr_h)):\n",
    "                curr_h[i].grad =  curr_hs[i].grad * sigmoid(curr_h[i]) * ( 1 - sigmoid(curr_h[i]))\n",
    "\n",
    "            #fix gradients in w_k with w_k[i][j].grad = s_(k-1)[j] * h_k[j].grad\n",
    "            #Or: grad of a weight d(input to weight layer) * 1 * grad after\n",
    "            curr_w = self.layers[k]\n",
    "            # print(self.hidden_states_sigmoid[k-1])\n",
    "            # print(curr_w, \"cur\")\n",
    "            for i in range(curr_w.shape[0]):\n",
    "                for j in range(curr_w.shape[1]):\n",
    "                    curr_w[i][j].grad = self.hidden_states_sigmoid[k-1][i] * self.hidden_states[k][j].grad \n",
    "                    \n",
    "            #Get gradients in b_k with b_k[i].grad = h_k[i].grad\n",
    "            curr_b = self.biases[k]\n",
    "            for i in range(curr_b.shape[0]):\n",
    "                curr_b[i].grad = self.hidden_states[k][i].grad \n",
    "                    \n",
    "            #Compute next s(k-1) gradients.\n",
    "            prev_hs = self.hidden_states_sigmoid[k - 1]\n",
    "            prev_h = self.hidden_states[k - 1]\n",
    " \n",
    "\n",
    "            for i in range(len(prev_hs)):\n",
    "                #Gonna have to do iteration i think\n",
    "                prev_hs[i].grad = 0\n",
    "                for j in range(len(curr_w[i])):\n",
    "                    prev_hs[i].grad += sigmoid(prev_h[i]) * ( 1 - sigmoid(prev_h[i])) * curr_w[i][j].grad\n",
    "\n",
    "\n",
    "        curr_h = self.hidden_states[0]\n",
    "        curr_hs = self.hidden_states_sigmoid[0]\n",
    "        for i in range(len(curr_h)):\n",
    "            curr_h[i].grad =  curr_hs[i].grad * sigmoid(curr_h[i]) * ( 1 - sigmoid(curr_h[i]))\n",
    "            \n",
    "        #Manual computation of the last layer that connects to input layer.\n",
    "        curr_w = self.layers[0]\n",
    "        for i in range(curr_w.shape[0]):\n",
    "            for j in range(curr_w.shape[1]):\n",
    "                curr_w[i][j].grad = self.input[j] * self.hidden_states[0][j].grad \n",
    "                \n",
    "        #Get gradients in b_k with b_k[i].grad = h_k[j].grad\n",
    "        curr_b = self.biases[0]\n",
    "        for j in range(curr_b.shape[0]):\n",
    "            curr_b[j].grad = 1 * self.hidden_states[0][j].grad \n",
    "\n",
    "        #Actual updates.\n",
    "        #Subtract gradient from everything. Here b and w refer to individual weights and biases inconsistent notation woohoo\n",
    "        for layer in self.layers:\n",
    "            for w in layer.flat:\n",
    "                 w -= w.grad * lr\n",
    "\n",
    "        for bias in self.biases:\n",
    "            for b in bias.flat:\n",
    "                b -= b.grad * lr\n",
    "                \n",
    "    def get_gradients(layer):\n",
    "        for weight in self.layers[i].flat:\n",
    "            weight.backprop(recursive=false)\n",
    "        for bias in self.biases[i].flat:\n",
    "            bias.backprop(recursive=false)  \n",
    "            \n",
    "          \n",
    "    def print_info(self, verbose=True):\n",
    "        print(\"layers \" )\n",
    "        for i in range(len(self.layers)):\n",
    "            print( f\"Layer {i} of shape {self.layers[i].shape}\")\n",
    "            print(self.layers[i])\n",
    "        print(\"biases \")\n",
    "        for i in range(len(self.biases)):\n",
    "            print( f\"Layer {i} of shape {self.biases[i].shape}\")\n",
    "            print(self.biases[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6dd0173-3186-43a9-a9c5-bdfbc7e15831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers \n",
      "Layer 0 of shape (4, 3)\n",
      "[[Number(0.0) Number(0.1) Number(0.2)]\n",
      " [Number(0.3) Number(0.4) Number(0.5)]\n",
      " [Number(0.6) Number(0.7) Number(0.8)]\n",
      " [Number(0.9) Number(1.0) Number(1.1)]]\n",
      "Layer 1 of shape (3, 5)\n",
      "[[Number(0.0) Number(0.1) Number(0.2) Number(0.3) Number(0.4)]\n",
      " [Number(0.5) Number(0.6) Number(0.7) Number(0.8) Number(0.9)]\n",
      " [Number(1.0) Number(1.1) Number(1.2) Number(1.3) Number(1.4)]]\n",
      "Layer 2 of shape (5, 2)\n",
      "[[Number(-0.3566467125909536) Number(0.4446689170495839)]\n",
      " [Number(0.021848321750071675) Number(-0.08533806000947641)]\n",
      " [Number(-0.23544438789537303) Number(0.27423368943421667)]\n",
      " [Number(-0.04384966778345145) Number(0.06843394886864851)]\n",
      " [Number(-0.48121019956364486) Number(0.11763549707587706)]]\n",
      "biases \n",
      "Layer 0 of shape (3,)\n",
      "[Number(0.0) Number(0.1) Number(0.2)]\n",
      "Layer 1 of shape (5,)\n",
      "[Number(0.0) Number(0.1) Number(0.2) Number(0.3) Number(0.4)]\n",
      "Layer 2 of shape (2,)\n",
      "[Number(-0.3817255741310668) Number(0.13992102132752382)]\n",
      "passed null check \n",
      "passed null check \n",
      "passed null check \n",
      "passed null check \n"
     ]
    }
   ],
   "source": [
    "tiny_x = [1, 2, 3, 4]\n",
    "tiny_test = Model(4, 2, [3, 5], stupid=True)\n",
    "tiny_test.print_info()\n",
    "tiny_test.train_epoch(tiny_x, np.array([2,3]))\n",
    "tiny_test.train_epoch(tiny_x, np.array([2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "734c4959-8b4f-4556-9c4a-26c7ed029f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def null_gradients(layers):\n",
    "    \"\"\"nulls all gradients\"\"\"\n",
    "    for layer in layers:\n",
    "        for weight in layer.flat:\n",
    "            weight.null_gradients()\n",
    "            \n",
    "def check_null(layers):\n",
    "    \"\"\"nulls all gradients\"\"\"\n",
    "    for layer in layers:\n",
    "        for weight in layer.flat:\n",
    "            if weight.grad != None:\n",
    "                print(\"hi welcome to another 5hrs of debuggin\")\n",
    "    print(\"passed null check \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b343a94-8b6b-4bef-b3e6-b86526c5a036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_data(x, y):\n",
    "    x = x.reshape(x.shape[0], 28*28)/255\n",
    "    test = np.zeros((x.shape[0], 10))\n",
    "    test[np.arange(x.shape[0]),y] = 1\n",
    "    return (x, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8b8477-4ff0-4fbc-b81f-b666cb8adaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), ds_info = tfds.load(\n",
    "    'mnist',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e13c3bf4-97dd-4ed4-b50d-dfaaaffa040d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data(path=\"mnist.npz\")\n",
    "testx, testy = fix_data(x_train[0:2], y_train[0:2])\n",
    "rawx = x_train[0:2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a38e3b1-1efa-4616-92ad-2887f58d8054",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_x, fixed_y = fix_data(x_train[:100], y_train[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae868f1-02dd-4716-8985-a6474b6d912e",
   "metadata": {},
   "outputs": [],
   "source": [
    "testa = np.array([[[2, 3], [4, 5]], [[2, 3], [4, 5]]])\n",
    "testa.reshape(2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97dd3072-b5be-45a9-a48d-a2636d2a38fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'weight_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Moment of truth\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m plsfuckingwork \u001b[38;5;241m=\u001b[39m Model(\u001b[38;5;241m28\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m10\u001b[39m, [\u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m32\u001b[39m])\n\u001b[0;32m      3\u001b[0m plsfuckingwork\u001b[38;5;241m.\u001b[39mtrain_epoch(fixed_x, fixed_y)\n",
      "Cell \u001b[1;32mIn[4], line 21\u001b[0m, in \u001b[0;36mModel.__init__\u001b[1;34m(self, input_size, output_size, hidden_layers, stupid)\u001b[0m\n\u001b[0;32m     18\u001b[0m prev \u001b[38;5;241m=\u001b[39m input_size\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hidden_layer \u001b[38;5;129;01min\u001b[39;00m hidden_layers:\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m# self.layers, weight_matrix([prev, hidden_layer])\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mappend(weight_matrix([prev, hidden_layer], stupid))\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbiases\u001b[38;5;241m.\u001b[39mappend(weight_matrix(hidden_layer, stupid))\n\u001b[0;32m     24\u001b[0m     prev  \u001b[38;5;241m=\u001b[39m hidden_layer\n",
      "\u001b[1;31mNameError\u001b[0m: name 'weight_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "#Moment of truth\n",
    "plsfuckingwork = Model(28*28, 10, [16, 32])\n",
    "plsfuckingwork.train_epoch(fixed_x, fixed_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d9e8a0ba-f789-4013-8d47-b90705af27f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred done \n",
      "mse calced \n",
      "nulled the layers\n",
      "finished nullig all gradiests\n",
      "finished backpropring gradiests\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d68dc17-213e-46eb-875c-92062094b361",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3037674f-af9f-40b2-9555-822b800dd9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y = torch.tensor(np.arange(6).reshape(3,2).astype(float), requires_grad=True)\n",
    "x = torch.tensor(np.arange(3).reshape(1,3).astype(float), requires_grad=True)\n",
    "out = x@y\n",
    "out.backward((torch.ones_like(out)))\n",
    "print(y.grad)\n",
    "print(x.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a98b73c-52d7-4cc6-9aff-1e46bc1ffa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_1 = weight_matrix((3, 2))\n",
    "matrix_2 = weight_matrix((1,3))\n",
    "print(\"t1 \", matrix_1)\n",
    "print(\"t2 \",matrix_2)\n",
    "\n",
    "test3 = matrix_2@matrix_1\n",
    "test4 = np.sum(test3)\n",
    "test4.null_gradients()\n",
    "test4.backprop()\n",
    "\n",
    "print(\"result\", test3)\n",
    "print(test4)\n",
    "\n",
    "print(\"grad\")\n",
    "print(get_grads(matrix_1))\n",
    "print(get_grads(matrix_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdd359d-b83f-46d4-b201-29b8af5b9cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(4)\n",
    "losses = []\n",
    "for _ in range(1000):\n",
    "    losses.append(train_epoch(model, data_set, l2_loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
