{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ada22e5-bc2e-4e08-b42e-e87ce41f8d49",
   "metadata": {},
   "source": [
    "# To Do:\n",
    "#i have a terrible idea: but wait wtf i dont tuple it later idk why its not working\n",
    "Figure out Jax\n",
    "Figure out fun with adagrad/etc\n",
    "Figure out cnn\n",
    "AND THEN WHAT I DID THIS ALL FOR! GRADIENT ASCENT BABY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a3b82e03-6a72-4875-8fb5-9fadb0360f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import jax.numpy as jnp\n",
    "from jax import grad\n",
    "import jax\n",
    "import numpy as np\n",
    "import math\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a84d4012-a431-4d5c-af5e-23d31c2c3ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_matrix(shape, naive=False):\n",
    "    \"\"\"weight matrix thingy.give dims. Not 0.\"\"\"\n",
    "    number = 1\n",
    "    if(type(shape) == int):\n",
    "        shape = [shape]\n",
    "    for i in shape:\n",
    "        number*= i\n",
    "    if naive:\n",
    "        return jnp.array([(i / 10) for i in range(number)]).reshape(*shape)\n",
    "    return np.array([np.random.uniform(low=-.2, high=.2, size=None) for i in range(number)]).reshape(*shape)\n",
    "    # return np.array([variable(np.random.uniform(low=-.2, high=.2, size=None)) for i in range(sizes[0] * sizes[1])).reshape(*shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "3edd1fd6-dd9e-470e-8f01-4c2325d3f006",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def sigmoid(x):\n",
    "    return jnp.vectorize(lambda x: 1/(1+math.e**-x))(x)\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return jnp.where(x <= 0, 1e-2 * x, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "b33f42b6-2ca4-46bc-bae6-9184d9b9d6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "class Model():\n",
    "    def __init__(self, input_size, output_size, hidden_layers, naive=False, seed=None):\n",
    "        '''\n",
    "        Takes list of # of things in their layers.\n",
    "        Layers are outputs?\n",
    "        '''\n",
    "        if seed != None:\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        self.layer_sizes = hidden_layers\n",
    "        self.layers = []\n",
    "        self.biases = []\n",
    "        \n",
    "        prev_size = input_size\n",
    "        \n",
    "        for hidden_layer in hidden_layers:\n",
    "            self.layers.append(weight_matrix([prev_size, hidden_layer], naive))\n",
    "            self.biases.append(weight_matrix(hidden_layer, naive))\n",
    "            prev_size  = hidden_layer\n",
    "            \n",
    "        self.biases.append(weight_matrix([output_size]))\n",
    "        self.layers.append(weight_matrix([prev_size, output_size]))\n",
    "\n",
    "        self.layers= tuple(self.layers)\n",
    "        self.biases = tuple(self.biases)  \n",
    " \n",
    "  \n",
    "    def fd(self, x):\n",
    "        '''f pass with input. '''\n",
    "\n",
    "        for i in range(len(self.layers)):\n",
    "            x = x @ self.layers[i]\n",
    "            x += self.biases[i]\n",
    "            if i != len(self.layers) - 1:\n",
    "                x = relu(x)\n",
    "            else:\n",
    "                x = sigmoid(x)\n",
    "            # self.hidden_states_activation.append(x)\n",
    "\n",
    "        return x\n",
    "        \n",
    "    def loss_static(self, params, x, y):\n",
    "        '''f pass with for loss.  '''\n",
    "        w, b = params\n",
    "        for i in range(len(b)):\n",
    "            x = x @ w[i]\n",
    "            x += b[i]\n",
    "            if i != len(b) - 1:\n",
    "                x = relu(x)\n",
    "            else:\n",
    "                x = sigmoid(x)\n",
    "\n",
    "        y = jnp.array(y)\n",
    "        return jnp.sum(x * x - 2 * x * y + y * y))\n",
    "\n",
    "            \n",
    "    def train_epoch(self, x, y, lr=10**-2):\n",
    "        '''\n",
    "        f pass and then uh gradient descent?\n",
    "\n",
    "        x:  \n",
    "        y: the goal. In not sparse tensor.\n",
    "        lr: how quick it learns\n",
    "        '''\n",
    "        losses = []\n",
    "        x = np.array(x)\n",
    "        \n",
    "        for batch_num in range(len(y)):\n",
    "            mse, grads = jax.value_and_grad(self.loss_static, argnums=(0))((self.layers, self.biases), x[batch_num], y[batch_num])\n",
    "            \n",
    "            losses.append(mse)\n",
    "            \n",
    "           #0 contains weights and 1 contains the bias grads. \n",
    "            # print([i.shape for i in grads[1]])\n",
    "            # print([i.shape for i in grads[0]])\n",
    "\n",
    "            #i have a terrible idea: but wait wtf i dont tuple it later idk why its not working\n",
    "            self.layers = list(self.layers)\n",
    "            self.biases = list(self.biases)\n",
    "            \n",
    "            for i, (layer, grad_layer) in enumerate(zip(self.layers, grads[0])):\n",
    "                self.layers[i] = layer - lr * grad_layer  \n",
    "                \n",
    "            for i, (bias, grad_bias) in enumerate(zip(self.biases, grads[1])):\n",
    "                self.biases[i] = bias - lr * grad_bias   \n",
    "                \n",
    "        preds = self.fd(x[batch_num]) \n",
    "        \n",
    "        correct = jnp.sum(jnp.argmax(preds, axis=1) == jnp.argmax(y[batch_num], axis=1))\n",
    "        acc = correct / len(y[batch_num])\n",
    "        print(f\"Acc: {acc} Loss: {mse}\")\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "1093bfdb-c57d-4f09-9ad5-b07d89877f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.5\n",
      "[0.  0.5 1.  1.5 2. ]\n"
     ]
    }
   ],
   "source": [
    "test = jnp.arange(5, dtype=float)\n",
    "test2 = jnp.arange(5, dtype=float)/2\n",
    "ihatethis(test, test2)\n",
    "\n",
    "val, grad = jax.value_and_grad(ihatethis)(test, test2)\n",
    "\n",
    "print(val)\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe71d89b-c6fb-4a9d-8eaa-f15e763836c4",
   "metadata": {},
   "source": [
    "# if this doesnt work ima cry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cdd3817b-2859-4488-aa24-952606bae576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9e3c7d2b-44bf-429b-818b-33a400c56644",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data(path=\"mnist.npz\", )\n",
    "indices = np.arange(len(x_train))\n",
    "np.random.shuffle(indices)\n",
    "x_train = x_train[indices]\n",
    "y_train = y_train[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8cdfd579-df78-4222-b44d-d6d4b477ecce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(x, y, batch_size=32):\n",
    "    if len(x) % batch_size != 0:\n",
    "        x = x[:batch_size * (len(x)//batch_size)]\n",
    "        y=y[:batch_size*(len(x)//batch_size)]\n",
    "    print(len(x)/batch_size)\n",
    "    return np.array(np.split(x, int(len(x) / batch_size), axis=0)), np.split(y, int(len(y)/batch_size), axis=0)\n",
    "\n",
    "def fix_data(x, y):\n",
    "    x = x.reshape(x.shape[0], 28*28)/255\n",
    "    test = np.zeros((x.shape[0], 10))\n",
    "    test[np.arange(x.shape[0]),y] = 1\n",
    "    return (x, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cddef3f9-0a00-4203-8ad7-4f376a4c83f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125, 4, 784)\n"
     ]
    }
   ],
   "source": [
    "print(b_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "3ca25994-ac22-460b-a803-cdd346687cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312.0\n"
     ]
    }
   ],
   "source": [
    "fixed_x, fixed_y = fix_data(x_train[:10000], y_train[:10000])\n",
    "b_x , b_y = batch(fixed_x, fixed_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "fb0e491d-46c3-4740-a153-657c959cfe34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.1875 Loss: 0.02851947210729122\n",
      "Acc: 0.1875 Loss: 0.028401657938957214\n",
      "Acc: 0.1875 Loss: 0.028073221445083618\n",
      "Acc: 0.21875 Loss: 0.026867175474762917\n",
      "Acc: 0.34375 Loss: 0.02428732067346573\n",
      "Acc: 0.46875 Loss: 0.022608567029237747\n",
      "Acc: 0.5625 Loss: 0.021389300003647804\n",
      "Acc: 0.6875 Loss: 0.02021365612745285\n",
      "Acc: 0.6875 Loss: 0.018730172887444496\n",
      "Acc: 0.71875 Loss: 0.01743321865797043\n"
     ]
    }
   ],
   "source": [
    "my_model = Model(28*28, 10, [4, 8])\n",
    "datas = []\n",
    "for _epoch in range(10):\n",
    "    datas.append(my_model.train_epoch(b_x, b_y, lr=10e-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b0d5a9-29d0-4a33-8ff4-145ca4db75a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline  \n",
    "\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "pic = 1\n",
    "for i, img in enumerate(x_test[:10]):\n",
    "  plt.subplot(2, 5, pic)\n",
    "  plt.axis('off')\n",
    "  predicted = my_model.fd(img.flat)\n",
    "  keras_pred = model.predict(img.reshape(1, 28*28))\n",
    "  plt.title(f\"T {y_test[i]} mine {np.argmax(predicted)} keras {np.argmax(keras_pred)} \")\n",
    "  plt.imshow(img)\n",
    "  pic+= 1\n",
    "plt.show()\n",
    "#60% acc. Considering this is from nearly scratch not terrible "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04591267-72f2-427e-bd16-da0eca6dd078",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
